{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BE/Bi 103, Fall 2015: Homework 6\n",
    "\n",
    "## Due 1pm, Monday, November 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This document was generated from a Jupyter notebook.  You can download the notebook [here](hw6.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "\n",
    "# Our numerical workhorses\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.misc\n",
    "import scipy.stats as st\n",
    "import scipy.special\n",
    "\n",
    "# The MCMC Hammer\n",
    "import emcee\n",
    "\n",
    "# Numba for speed\n",
    "import numba\n",
    "\n",
    "# BE/Bi 103 utilities\n",
    "import bebi103\n",
    "\n",
    "# Import plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import corner\n",
    "\n",
    "# Magic function to make matplotlib inline; other style specs must come AFTER\n",
    "%matplotlib inline\n",
    "\n",
    "# This enables high res graphics inline (only use with static plots (non-Bokeh))\n",
    "# SVG is preferred, but there is a bug in Jupyter with vertical lines\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# JB's favorite Seaborn settings for notebooks\n",
    "rc = {'lines.linewidth': 2, \n",
    "      'axes.labelsize': 18, \n",
    "      'axes.titlesize': 18, \n",
    "      'axes.facecolor': 'DFDFE5'}\n",
    "sns.set_context('notebook', rc=rc)\n",
    "sns.set_style('darkgrid', rc=rc)\n",
    "\n",
    "# Import Bokeh modules for interactive plotting\n",
    "import bokeh.charts\n",
    "import bokeh.charts.utils\n",
    "import bokeh.io\n",
    "import bokeh.models\n",
    "import bokeh.palettes\n",
    "import bokeh.plotting\n",
    "\n",
    "# Display graphics in this notebook\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "# Suppress future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6.1: Microtubule catastrophe, 70 pts + 15 pts extra credit\n",
    "\n",
    "In [Homework 1](hw1.html), we plotted data of microtubule catastrophe times.  In this problem, we return to the data from the [Gardner, Zanic, et al. paper](../protected/papers/gardner_2011.pdf)  We will carefully analyze the data and make some conclusions about the processes underlying microtubule catastrophe.  You can download the data set [here](../protected/data/gardner_hw6.zip).\n",
    "\n",
    "In the file `gardner_mt_catastrophe_only_tubulin.csv`, we have observed catastrophe times of microtubules with different concentrations of tubulin.  So, our data set $D$ consists of a set of measurements of the amount of time to catastrophe; $D = \\{t_i\\}$.  To model microtuble catastrophe, we will assume that a series of $m$,  processes must occur sequentially to trigger catastrophe.  This could be separate depolymerization events, binding of auxiliary proteins, etc.  We assume that each of these processes is a Poisson process, and that process $j$ occurs with rate $1/\\tau_j$.  Our goal here is to do model selection to determine the value of $m$.\n",
    "\n",
    "As we have learned, the model specifies the likelihood.  We will assume all microtubule catastrophes are independent, so the likelihood for all of our observed catastrophes \n",
    "\n",
    "\\begin{align}\n",
    "P(D\\mid \\boldsymbol{\\tau}, m, I) = \\prod_i P(t_i\\mid \\boldsymbol{\\tau}, m, I).\n",
    "\\end{align}\n",
    "\n",
    "**a)** Explain why the probability distribution for catastrophe times for a three-step process ($m=3$) is\n",
    "\n",
    "\\begin{align}\n",
    "P(t\\mid \\tau_1, \\tau_2, \\tau_3, 3, I) = \\frac{1}{\\tau_1\\tau_2\\tau_3}\\int_0^t\\mathrm{d}t_1 \\int_{t_1}^t\\mathrm{d}t_2\\, \\mathrm{e}^{-t_1/\\tau_1}\\,\\mathrm{e}^{-(t_2-t_1)/\\tau_2}\\,\\mathrm{e}^{-(t-t_2)/\\tau_3}.\n",
    "\\end{align}\n",
    "\n",
    "**b)** The above expression for general $m$ can be integrated, giving\n",
    "\n",
    "\\begin{align}\n",
    "P(t\\mid \\boldsymbol{\\tau}, m, I) = \\sum_{j=1}^m \\frac{\\tau_j^{m-2}\\,\\mathrm{e}^{-t/\\tau_j}}{\\prod_{k=1,k\\ne j}^m (\\tau_j - \\tau_k)}.\n",
    "\\end{align}\n",
    "\n",
    "For clarity, the probability distributions for the first few $m$ are\n",
    "\n",
    "\\begin{align}\n",
    "P(t\\mid \\tau_1, 1, I) &= \\frac{\\mathrm{e}^{-t/\\tau_1}}{\\tau_1},\\\\[1em]\n",
    "P(t\\mid \\tau_1, \\tau_2, 2, I) &=\n",
    "\\frac{\\mathrm{e}^{-t/\\tau_1}}{\\tau_1 - \\tau_2} + \\frac{\\mathrm{e}^{-t/\\tau_2}}{\\tau_2 - \\tau_1}\n",
    "= \\frac{\\mathrm{e}^{-t/\\tau_2} - \\mathrm{e}^{-t/\\tau_1}}{\\tau_2 - \\tau_1} \\\\[1em]\n",
    "P(t\\mid \\tau_1, \\tau_2, \\tau_3, 3, I) &=\n",
    "\\frac{\\tau_1\\,\\mathrm{e}^{-t/\\tau_1}}{(\\tau_1 - \\tau_2)(\\tau_1-\\tau_3)}\n",
    "+\\frac{\\tau_2\\,\\mathrm{e}^{-t/\\tau_2}}{(\\tau_2 - \\tau_1)(\\tau_2-\\tau_3)}\n",
    "+\\frac{\\tau_3\\,\\mathrm{e}^{-t/\\tau_3}}{(\\tau_3 - \\tau_1)(\\tau_3-\\tau_2)}\n",
    "\\end{align}\n",
    "\n",
    "Note that these probability distributions assume that no two of the $\\tau_j$'s are equal, and you should explicitly ensure this in your calculations.  If any two $\\tau_j$'s are equal, you need to take a limit, e.g.,\n",
    "\n",
    "\\begin{align}\n",
    "\\lim_{\\tau_2\\to\\tau_1} P(t\\mid \\tau_1, \\tau_2, 2, I) &= \\frac{t^2}{2\\tau_1}\\,\\mathrm{e}^{-t/\\tau_1},\n",
    "\\end{align}\n",
    "\n",
    "in this case, as gamma distribution.  Not to worry; we will not include this limit in our analysis here.\n",
    "\n",
    "In fact, you should specify $\\tau_1 < \\tau_2 < \\cdots < \\tau_n$.  Why is this ok to do, and why should you do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** For the trials where the tubulin concentration is 12 µM (from the file `gardner_mt_catastrophe_only_tubulin.csv`), perform parameter estimation for the $\\{\\tau_j\\}$ and model selection for various values of $m$.  Report the results with clear graphics and discussion.  \n",
    "\n",
    "*Hint*: Computing the log of the sum of exponentials while dealing with machine precision is a tricky business, and something that comes up often in probability.  Here is my advice.  Say you are computing $\\ln(\\mathrm{e}^a - \\mathrm{e}^b + \\mathrm{e}^c)$ with $a>b>c$.  We have\n",
    "\n",
    "\\begin{align}\n",
    "\\ln(\\mathrm{e}^a - \\mathrm{e}^b + \\mathrm{e}^c) = \\ln\\left(\\mathrm{e}^a(1 - \\mathrm{e}^{b-a} + \\mathrm{e}^{c-a})\\right)\n",
    "= a + \\ln(1 - \\mathrm{e}^{b-a} + \\mathrm{e}^{c-a}).\n",
    "\\end{align}\n",
    "\n",
    "This latter expression is much easier to compute numerically because all entries in the sum inside the logarithm at less than or equal to one, given that $a$ is the largest argument to the exponentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Using whichever model you found most probable when you computed the odds ratio for the 12 µM tubulin measurements, the values of the $\\tau_j$'s for the other concentrations of tubulin.  Given that microtubules polymerize faster with higher tubulin concentrations, is there anything you can say about the occurrence of catastrophe by looking at the values of the $\\tau_j$'s versus tubulin concentration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) (15 points extra credit)** In the files `gardner_mt_catastrophe_kip3.csv` and `gardner_mt_catastrophe_mcak.csv`, there are measurements of catastrophe times in the presence of the kinesins Kip3 and MCAK with 12 µM tubulin.  Analyze these data and discuss conclusions about their respective roles in microtubule catastrophe.  *Note*: This part of the problem is intentionally open-ended.  You should think carefully, and perform a complete analysis to draw your conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6.1: solution\n",
    "\n",
    "**a)** To be added.\n",
    "\n",
    "**b)** If we order the $\\tau_j$'s, we can write\n",
    "\n",
    "\\begin{align}\n",
    "P(t\\mid \\boldsymbol{\\tau}, m, I) = \\sum_{j=1}^m (-1)^{m-j}\\,\\frac{\\tau_j^{m-2}\\,\\mathrm{e}^{-t/\\tau_j}}{\\prod_{k=1,k\\ne j}^m \\left|\\tau_j - \\tau_k\\right|}.\n",
    "\\end{align}\n",
    "\n",
    "This enables us to write the sum as\n",
    "\n",
    "\\begin{align}\n",
    "P(t\\mid \\boldsymbol{\\tau}, m, I)  = \\sum_{j=1}^m (-1)^{m-j}\\, \\exp\\left\\{-\\frac{t}{\\tau_j} + (m-2) \\ln \\tau_j - \\sum_{k=1,k\\ne j}^m \\ln\\left|\\tau_j-\\tau_k\\right|\\right\\}.\n",
    "\\end{align}\n",
    "\n",
    "The likelihood is then\n",
    "\n",
    "\\begin{align}\n",
    "P(D\\mid \\boldsymbol{\\tau}, m, I) = \\prod_i\\left(\n",
    "\\sum_{j=1}^m (-1)^{m-j}\\, \\exp\\left\\{-\\frac{t_i}{\\tau_j} + (m-2) \\ln \\tau_j - \\sum_{k=1,k\\ne j}^m \\ln\\left|\\tau_j-\\tau_k\\right|\\right\\}\n",
    "\\right),\n",
    "\\end{align}\n",
    "\n",
    "giving a log likelihood of\n",
    "\n",
    "\\begin{align}\n",
    "\\ln P(D\\mid \\boldsymbol{\\tau}, m, I) = \\sum_i \\ln\\left(\\sum_{j=1}^m (-1)^{m-j}\\, \\exp\\left\\{-\\frac{t_i}{\\tau_j} + (m-2) \\ln \\tau_j - \\sum_{k=1,k\\ne j}^m \\ln\\left|\\tau_j-\\tau_k\\right|\\right\\}\\right).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** First, let's load in the data, tidy it, and extract our times of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "fname = '../data/gardner_et_al/gardner_mt_catastrophe_only_tubulin.csv'\n",
    "df = pd.read_csv(fname, comment='#')\n",
    "\n",
    "# Tidy the DataFrame\n",
    "df = pd.melt(df, var_name='tubulin conc (µM)', \n",
    "             value_name='time to catastrophe (s)')\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert concentrations from string to numbers\n",
    "df['tubulin conc (µM)'] = \\\n",
    "        df['tubulin conc (µM)'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Pull our t, and have it sorted for convenience\n",
    "t = df[df['tubulin conc (µM)']==12]['time to catastrophe (s)'].values\n",
    "t.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot an ECDF just so we can see what we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct ECDF\n",
    "y = np.arange(1, len(t) + 1)\n",
    "plt.plot(t, y, '.')\n",
    "plt.xlabel('time to catastrophe (s)')\n",
    "plt.ylabel('ECDF')\n",
    "plt.margins(0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll define our log likelihood.  Importantly, we need to use the logsumexp trick to compute the log of the sum of exponentials.  Otherwise, we will run into trouble with underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(tau, t):\n",
    "    \"\"\"\n",
    "    Log likelihood using sp.misc.logsumexp()\n",
    "    \"\"\"\n",
    "    # Number of data points\n",
    "    n = len(t)\n",
    "    \n",
    "    # Number of Poisson processes\n",
    "    m = len(tau)\n",
    "    \n",
    "    # Compute special case first\n",
    "    if m == 1:\n",
    "        return -n * np.log(tau[0]) - t.sum() / tau[0]\n",
    "    else:\n",
    "        # Scale for logsumexp\n",
    "        b = np.empty((1, m), dtype=int)\n",
    "        if (m % 2) == 0:\n",
    "            b[0,::2] = -1\n",
    "            b[0,1::2] = 1\n",
    "        else:\n",
    "            b[0,::2] = 1\n",
    "            b[0,1::2] = -1    \n",
    "\n",
    "        # Set up arguments of exponentials in sum\n",
    "        exp_args = np.empty((n, m), dtype=np.float)\n",
    "        for j, tau_j in enumerate(tau):\n",
    "            sum_logs = np.sum(np.log(np.abs(tau[:j] - tau[j]))) \\\n",
    "                            + np.sum(np.log(np.abs(tau[j+1:] - tau[j])))\n",
    "            exp_args[:,j] = -t / tau_j + (m - 2) * np.log(tau_j) - sum_logs\n",
    "\n",
    "        return scipy.misc.logsumexp(exp_args, b=b, axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log likelihood is a complicated function.  It is generally a good idea with any function, especially a complicated one, to test it for simple test cases.  We'll therefore write a testing function that works for $m \\in \\{1,2,3,4\\}$.  The test function does not explicitly deal with the precision issues, but if we choose the right $\\tau$'s, it will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_log_likelihood(tau, t):\n",
    "    \"\"\"\n",
    "    Check log likelihood function for first few values of m\n",
    "    \"\"\"\n",
    "    # Number of data points\n",
    "    n = len(t)\n",
    "        \n",
    "    # Number of Poisson processes\n",
    "    m = len(tau)\n",
    "    \n",
    "    # Compute log likelihood for special cases\n",
    "    if m == 1:\n",
    "        log_like = -n * np.log(tau[0]) - t.sum() / tau[0]\n",
    "    \n",
    "    if m == 2:\n",
    "        log_like = np.sum(np.log(\n",
    "                (np.exp(-t/tau[1]) - np.exp(-t/tau[0])) / (tau[1] - tau[0])))\n",
    "    \n",
    "    if m == 3:\n",
    "        log_like = np.sum(np.log(\n",
    "            np.exp(-t/tau[0]) * tau[0] / (tau[0] - tau[1]) / (tau[0] - tau[2]) \\\n",
    "          + np.exp(-t/tau[1]) * tau[1] / (tau[1] - tau[0]) / (tau[1] - tau[2]) \\\n",
    "          + np.exp(-t/tau[2]) * tau[2] / (tau[2] - tau[0]) / (tau[2] - tau[1])))\n",
    "    \n",
    "    if m == 4:\n",
    "        tau_prods = [(tau[0] - tau[1]) * (tau[0] - tau[2]) * (tau[0] - tau[3]),\n",
    "                     (tau[1] - tau[0]) * (tau[1] - tau[2]) * (tau[1] - tau[3]),\n",
    "                     (tau[2] - tau[0]) * (tau[2] - tau[1]) * (tau[2] - tau[3]),\n",
    "                     (tau[3] - tau[0]) * (tau[3] - tau[1]) * (tau[3] - tau[2])]\n",
    "        log_like =  np.sum(np.log(\n",
    "                np.exp(-t/tau[0]) * tau[0]**2 / tau_prods[0] \\\n",
    "              + np.exp(-t/tau[1]) * tau[1]**2 / tau_prods[1] \\\n",
    "              + np.exp(-t/tau[2]) * tau[2]**2 / tau_prods[2] \\\n",
    "              + np.exp(-t/tau[3]) * tau[3]**2 / tau_prods[3]))\n",
    "    \n",
    "    # Make sure we got the right values\n",
    "    if np.isfinite(log_like):\n",
    "        assert np.isclose(log_likelihood(tau, t), log_like)\n",
    "        assert np.isclose(log_likeihood_sp(tau, t), log_like)\n",
    "    else:\n",
    "        print('Infinite log likelihood test value.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the test function, we can try it for dummy values of `t` and `tau`.  If no `AssertionError` come up, then we know it passed the tests.  We'll generate 20 randome times, and then try 100 random sets of $\\tau$'s for each $m$ that we test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_t = np.random.uniform(0, 10, 20)\n",
    "\n",
    "for m in [2, 3, 4]:\n",
    "    test_tau = np.random.uniform(0, 1, (100, m))\n",
    "    test_tau = np.sort(test_tau, axis=1)\n",
    "    for i in range(len(test_tau)):\n",
    "        test_log_likelihood(test_tau[i,:], test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!  Our function works as we expect, so we will go forward using it.\n",
    "\n",
    "We next need to define the prior.  We need to make sure the $\\tau$'s are rank ordered and positive.  If they are, we take a Jeffreys prior for each $\\tau$, which we consider to be independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_prior(tau, tau_min, tau_max):\n",
    "    \"\"\"\n",
    "    Log prior for successive Poisson processes model.\n",
    "    \"\"\"\n",
    "    if (tau[0] < tau_min) or (tau[-1] > tau_max) \\\n",
    "                        or (not (np.diff(tau) > 0).all()):\n",
    "        return -np.inf\n",
    "    \n",
    "    return -len(tau) * np.log(np.log(tau_max / tau_min)) - np.log(tau).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a log likelihood and a log prior.  We just need to get initial conditions for walkers, and we can perform MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def walker_starting_points(m, n_temps, n_walkers, tau_min, tau_max):\n",
    "    \"\"\"\n",
    "    Generate walker starting positions for PTMCMC\n",
    "    \"\"\"\n",
    "    p0 = np.random.uniform(tau_min, tau_max, (n_temps, n_walkers, m))\n",
    "    p0 = np.sort(p0, axis=2)\n",
    "    return p0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll write a function to do the PTMCMC sampling for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_ptmcmc(t, m, tau_min=1, tau_max=2000, n_temps=20, n_walkers=100, \n",
    "                  n_burn=100, n_steps=500, threads=None):\n",
    "\n",
    "    # Starting points of walkers\n",
    "    p0 = walker_starting_points(m, n_temps, n_walkers, tau_min, tau_max)\n",
    "    \n",
    "    # Columns for output DataFrame\n",
    "    columns = ['tau_' + str(i) + ' (s)' for i in range(1, m+1)]\n",
    "    \n",
    "    return bebi103.run_pt_emcee(\n",
    "        log_likelihood, log_prior, n_burn, n_steps, n_temps=n_temps, p0=p0, \n",
    "        loglargs=(t,), logpargs=(tau_min, tau_max), threads=threads, \n",
    "        columns=columns, return_lnZ=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up a dictionary for the output.  We'll test up to $m_\\mathrm{max} = 6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_max = 6\n",
    "results = {m: {} for m in range(1, m_max+1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first do parameter estimation for a single Poisson process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for m in range(1, m_max+1):\n",
    "    print('Performing MCMC for m =', m, '...', flush=True)\n",
    "    results[m]['df'], results[m]['ln_Z'], results[m]['dln_Z'] = sample_ptmcmc(\n",
    "            t, m, tau_min=1, tau_max=2000, n_temps=20, n_walkers=100, \n",
    "            n_burn=200, n_steps=1000, threads=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the sampling is done, let's check the odds ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for m in range(2, m_max+1):\n",
    "    print('O_1{0:d} = {1:.2e}'.format(\n",
    "            m, (results[1]['ln_Z'] - results[m]['ln_Z'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('ptmcmc_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1 = results[1]['df'].copy()\n",
    "corner.corner(df_1[df_1.beta_ind==0][['tau_1 (s)']], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2 = results[2]['df'].copy()\n",
    "corner.corner(df_2[df_2.beta_ind==0][['tau_1 (s)', 'tau_2 (s)']], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_3 = results[3]['df'].copy()\n",
    "corner.corner(df_3[df_3.beta_ind==0][['tau_1 (s)', 'tau_2 (s)', 'tau_3 (s)']], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_4 = results[4]['df'].copy()\n",
    "corner.corner(df_4[df_4.beta_ind==0][['tau_1 (s)', 'tau_2 (s)', 'tau_3 (s)', 'tau_4 (s)']], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_5 = results[5]['df'].copy()\n",
    "corner.corner(df_5[df_5.beta_ind==0][['tau_1 (s)', 'tau_2 (s)', 'tau_3 (s)', 'tau_4 (s)', 'tau_5 (s)']], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_6 = results[6]['df'].copy()\n",
    "corner.corner(df_6[df_6.beta_ind==1][['tau_1 (s)', 'tau_2 (s)', 'tau_3 (s)', 'tau_4 (s)', 'tau_5 (s)', 'tau_6 (s)']], bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6.2: Spike sorting, 30 pts\n",
    "\n",
    "Dawna and Kyu kindly provided us with another set of voltage measurements from a mouse retina.  You can download the data [here](../data/H930start2filt.txt.zip).  These data have been pre-filtered, so you do not need to do any filtering, unless you think it will help your analysis.  For this trace:\n",
    "\n",
    "**a)** Locate all spikes.\n",
    "\n",
    "**b)** There are two types of spikes.  Devise a way to automatically tell the difference between each type of spike.  Plot all of the spikes overlayed on top of each other with their minima at the same point.  The plot should be color-coded so that two classes of spikes have different colors.\n",
    "\n",
    "**c)** Plot the probability distributions of inter-spike times for each type of spike and comment on anything you see of note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6.2: solution\n",
    "**a)** We start by loading in the data set and converting to NumPy arrays for faster indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/bagherian_et_al/H930start2filt.txt', sep='\\t',\n",
    "                 comment='#', header=None, names=['time (ms)', 'V (µV)'])\n",
    "\n",
    "t = df['time (ms)'].values\n",
    "V = df['V (µV)'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(t, V, '-')\n",
    "plt.xlabel('time (ms)')\n",
    "plt.ylabel('V (µV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be two types of spikes, a deep one and a shallow one.  This is easier seen with Bokeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = bokeh.charts.Line(df.loc[10000:30000,:], x='time (ms)', y='V (µV)', \n",
    "                      color='slateblue')\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In zooming around the above plot, we can see to spikes between 2.70 and 2.73 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inds = (df['time (ms)'] > 2700) & (df['time (ms)'] < 2730)\n",
    "p = bokeh.charts.Line(df.loc[inds,:], x='time (ms)', y='V (µV)', \n",
    "                      color='slateblue')\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first plot, it appears as though all spikes cross a threshold of -60 µV, so we will identify spikes by downward crossings of -60 µV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresh = -60  # µV\n",
    "\n",
    "# Find indices of downward crossings\n",
    "crossing_inds = np.where(np.logical_and(V[:-1] > thresh, V[1:] < thresh))[0] + 1\n",
    "\n",
    "# How many spikes?\n",
    "print(len(crossing_inds), 'spikes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.diff(crossing_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(max(len(down), len(up))):\n",
    "    r = [down[i] - 10, up[i]+10]\n",
    "    plt.plot(V[r[0]:r[1]], 'k-', lw=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(t, V, '-')\n",
    "plt.plot(t[down+1], V[down+1], 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
