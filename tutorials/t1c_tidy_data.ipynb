{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1c: Tidy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This tutorial was generated from an Jupyter notebook.  You can download the notebook [here](t1b_tidy_data.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import collections\n",
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Our numerical workhorses\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import scipy.signal\n",
    "\n",
    "# Import pyplot for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seaborn, useful for graphics\n",
    "import seaborn as sns\n",
    "\n",
    "# Import Bokeh modules for interactive plotting\n",
    "import bokeh.io\n",
    "import bokeh.models\n",
    "import bokeh.plotting\n",
    "\n",
    "# Magic function to make matplotlib inline; other style specs must come AFTER\n",
    "%matplotlib inline\n",
    "\n",
    "# This enables SVG graphics inline (only use with static plots (non-Bokeh))\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# JB's favorite Seaborn settings for notebooks\n",
    "rc = {'lines.linewidth': 2, \n",
    "      'axes.labelsize': 18, \n",
    "      'axes.titlesize': 18, \n",
    "      'axes.facecolor': 'DFDFE5'}\n",
    "sns.set_context('notebook', rc=rc)\n",
    "sns.set_style('darkgrid', rc=rc)\n",
    "\n",
    "# Set up Bokeh for inline viewing\n",
    "# bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying using Pandas\n",
    "We can use Pandas powerful tools to tidy our data by loading it into a Pandas `DataFrame`, and then use some of Pandas's slick functions to make it tidy.\n",
    "\n",
    "We start by loading in the metadata describing the regions for each of the grayordinates in the main data file.  We load them in as Pandas `Series` objects because they are easier to index (no need to specify a column name).  We do this by using the `pd.read_csv()` function to read into a `DataFrame`, and then just slide out the `Series` we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the key that relates brain region to identifies\n",
    "s_region_keys = pd.read_csv('../data/dubois_et_al/full_dataset/regionkey.csv', \n",
    "                            header=None, names=['region_name'], \n",
    "                            index_col=0)['region_name']\n",
    "\n",
    "# Load in regions corresponding to each row of big data set\n",
    "s_regions = pd.read_csv('../data/dubois_et_al/full_dataset/SUB01_region.csv', \n",
    "                        header=None, names=['region_id'])['region_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to be careful because the data set has some regions listed as \"`0`,\" which means that the brain region is undefined.  This is not included in `s_regions_keys`, so we need to add that in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load in the main `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in big daddy\n",
    "df = pd.read_csv('../data/dubois_et_al/full_dataset/SUB01_FIX_data.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `pd.melt()` to put it in tidy format.  Here is our strategy\n",
    "\n",
    "1. Compute the time points, knowing that each column in the `DataFrame` holds a single time point, and that the elapsed time between samples is one second.\n",
    "2. Make the time points the column names.\n",
    "3. Concatenate the region index onto the `DataFrame`.\n",
    "4. Concatenate the region string onto the `DataFrame`.\n",
    "5. Melt the `DataFrame`.\n",
    "6. Write it out as a compressed CSV file.\n",
    "\n",
    "We'll do steps 1-5 in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Frame rate\n",
    "frame_rate = 1.0  # frames per second\n",
    "\n",
    "# Number of frames (equal to number of columns)\n",
    "n_frames = len(df.columns)\n",
    "\n",
    "# Compute time points of images\n",
    "t = np.linspace(0.0, (n_frames - 1) / frame_rate, n_frames)\n",
    "\n",
    "# Make the time points into the column names\n",
    "df.columns = t\n",
    "\n",
    "# Put in the region index\n",
    "df['region_id'] = s_regions\n",
    "\n",
    "# Add region names\n",
    "reg_list = [s_region_keys[region] for region in s_regions]\n",
    "df['region_name'] = reg_list\n",
    "\n",
    "# The index if the grayordinate; keep them\n",
    "df['grayordinate'] = df.index\n",
    "\n",
    "# Melt the DataFrame\n",
    "df = pd.melt(df, id_vars=['region_id', 'region_name', 'grayordinate'], \n",
    "             var_name='time (s)', value_name='voxel_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a tidy `DataFrame`!  We can write it out to a CSV file so we have it for later.  One issue with tidy data is that storing it in raw form results in bloated file sizes because of the many repeated values.  However, having many repeated values does enable good compression.  We can therefore use the [gzip module](https://docs.python.org/3/library/gzip.html) from the standard Python library to make the new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Write CSV\n",
    "# df.to_csv('../data/dubois_et_al/SUB01_data_tidy.csv', index=False, \n",
    "#           float_format='%.5f')\n",
    "\n",
    "# # Compress it\n",
    "# with open('../data/dubois_et_al/SUB01_data_tidy.csv', 'rb') as f_in:\n",
    "#     with gzip.open('../data/dubois_et_al/SUB01_data_tidy.csv.gz', 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "# # Delete uncompressed CSV\n",
    "# os.remove('../data/dubois_et_al/SUB01_data_tidy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the relative size of the compressed original file and the compressed tidy file, the tidy file is only about twice as big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# os.path.getsize('../data/dubois_et_al/SUB01_data_tidy.csv.gz') / \\\n",
    "#             os.path.getsize('../data/dubois_et_al/SUB01_data.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying by hand\n",
    "Another way to generate a tidy `DataFrame` is to hand build a CSV file while reading in the data.  The advantage of this approach is that we do not have to read in the entire original data set.  In fact, only one line at a time is read in and written out, so we will not have any issues with RAM.  The output is also a bit prettier, because we can control the ordering of the columns and rows.  However, this does not really matter when working with tidy data, since the indices and ordering is irrelevant. \n",
    "\n",
    "We will reload the `DataFrame`s with the regions and region keys so that the code block below can stand alone without everything we did in the [previous section](#Tidying-using-Pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Load in the key that relates brain region to identifies\n",
    "# s_region_keys = pd.read_csv('../data/dubois_et_al/regionkey.csv', \n",
    "#                             header=None, names=['region_name'], \n",
    "#                             index_col=0)['region_name']\n",
    "\n",
    "# # Load in regions corresponding to each row of big data set\n",
    "# s_regions = pd.read_csv('../data/dubois_et_al/SUB01_region.csv', \n",
    "#                         header=None, names=['region_id'])['region_id']\n",
    "\n",
    "# # Append zero region key for undefined region\n",
    "# s_region_keys[0] = 'UNDEFINED'\n",
    "\n",
    "# # Frame rate\n",
    "# frame_rate = 1.0  # frames per second\n",
    "\n",
    "# infile = '../data/dubois_et_al/SUB01_data.csv.gz'\n",
    "# outfile = '../data/dubois_et_al/SUB01_data_tidy.csv'\n",
    "# with gzip.open(infile, 'r') as f_in, open(outfile, 'w') as f_out:\n",
    "#     # Write header\n",
    "#     f_out.write('grayordinate,region_id,region_name,time (s),voxel_value\\n')\n",
    "    \n",
    "#     # Loop through time entries in data file  (each one is a grayordinate)\n",
    "#     g = 0\n",
    "#     line = f_in.readline()\n",
    "#     while line != b'':\n",
    "#         # Determine region ID, region name, and voxel values for time series\n",
    "#         region_id = s_regions[g]\n",
    "#         region_name = s_region_keys[region_id]\n",
    "#         voxel_vals = np.fromstring(line, sep=',')\n",
    "\n",
    "#         # Compute time points for time series\n",
    "#         t = np.linspace(0.0, (len(voxel_vals) - 1) / frame_rate, len(voxel_vals))\n",
    "\n",
    "#         # Write time series to CSV file in tidy format\n",
    "#         for i, val in enumerate(voxel_vals):\n",
    "#             f_out.write('%d,%d,%s,%g,%.5f\\n' \\\n",
    "#                                 % (g, region_id, region_name, t[i], val))\n",
    "        \n",
    "#         # Go to the next grayordinate\n",
    "#         g += 1\n",
    "#         line = f_in.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have made the file, we can compress it, as we did before.  We will overwrite the one we made in the [previous section]([previous section](#Tidying-using-Pandas), because it does not really matter which one we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Compress it\n",
    "# with open('../data/dubois_et_al/SUB01_data_tidy.csv', 'rb') as f_in:\n",
    "#     with gzip.open('../data/dubois_et_al/SUB01_data_tidy.csv.gz', 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "# # Delete uncompressed CSV\n",
    "# os.remove('../data/dubois_et_al/SUB01_data_tidy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, storing the data in tidy format is advantageous because now we don't have to re-tidy it after loading it in.  Another advantage is that we can now use a tool like [Dask](http://dask.pydata.org/en/latest/) to handle the large data set taking advantage of parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean activity of each region\n",
    "Compute mean activity of each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gb_names = ['region_id', 'region_name', 'time (s)']\n",
    "df_reg = df.groupby(gb_names)['voxel_value'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Columns to consider\n",
    "regs = list(df_reg['region_id'].unique())\n",
    "regs.pop(0)\n",
    "\n",
    "# DataFrame to hold correlations\n",
    "cols = ['region_1', 'region_2', 'region_1_name', 'region_2_name', 'pearson_r']\n",
    "df_corr = pd.DataFrame(columns=cols)\n",
    "\n",
    "n = len(df_reg['region_id'].unique())\n",
    "# Compute pearson correlation\n",
    "for r1 in regs:\n",
    "    for r2 in regs:\n",
    "        r1_name = s_region_keys[r1]\n",
    "        r2_name = s_region_keys[r2]\n",
    "        r = st.pearsonr(df_reg[df_reg['region_id']==r1]['voxel_value'], \n",
    "                         df_reg[df_reg['region_id']==r2]['voxel_value'])[0]\n",
    "        df_corr = df_corr.append(pd.DataFrame([[r1, r2, r1_name, r2_name, r]], \n",
    "                                              columns=cols), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rgb_frac_to_hex(rgb_frac):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    \"\"\"\n",
    "    return '#{0:02x}{1:02x}{2:02x}'.format(int(rgb_frac[0] * 255), \n",
    "                                           int(rgb_frac[1] * 255),\n",
    "                                           int(rgb_frac[2] * 255))\n",
    "\n",
    "\n",
    "def data_to_hex_color(data, palette, data_range=[-1, 1]):\n",
    "    \"\"\"\n",
    "    Convert a data point to a color\n",
    "    \"\"\"\n",
    "    if data > data_range[1] or data < data_range[0]:\n",
    "        raise RuntimeError('data outside of range')\n",
    "    elif data == data_range[1]:\n",
    "        return rgb_frac_to_hex(palette[-1])\n",
    "    \n",
    "    f = (data - data_range[0]) / (data_range[1] - data_range[0])\n",
    "    return rgb_frac_to_hex(palette[int(f * len(palette))])\n",
    "    \n",
    "\n",
    "def plot_mat(df, i_col, j_col, data_col, n_colors=21, colormap='RdBu_r'):\n",
    "    \"\"\"\n",
    "    Plot matrix.\n",
    "    \"\"\"\n",
    "    # Get colors\n",
    "    palette = sns.color_palette(colormap, n_colors)\n",
    "    \n",
    "    # Compute colors for squares\n",
    "    df['color'] = df[data_col].apply(data_to_hex_color, args=(palette,))\n",
    "    \n",
    "    # Data source\n",
    "    source = bokeh.plotting.ColumnDataSource(df)\n",
    "    \n",
    "    tools = 'reset,resize,hover,save,pan,box_zoom,wheel_zoom'\n",
    "\n",
    "    p = bokeh.plotting.figure(\n",
    "               x_range=list(df[i_col].unique()),\n",
    "               y_range=list(reversed(list(df[j_col].unique()))),\n",
    "               x_axis_location='above', plot_width=1000, plot_height=1000,\n",
    "               toolbar_location='left', tools=tools)\n",
    "\n",
    "    p.rect(i_col, j_col, 1, 1, source=source, color='color', line_color=None)\n",
    "\n",
    "    p.grid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = '8pt'\n",
    "    p.axis.major_label_standoff = 0\n",
    "    p.xaxis.major_label_orientation = np.pi/3\n",
    "\n",
    "    hover = p.select(dict(type=bokeh.models.HoverTool))\n",
    "    hover.tooltips = collections.OrderedDict([\n",
    "    ('i', '  @' + i_col),\n",
    "    ('j', '  @' + j_col),\n",
    "    (data_col, '  @' + data_col)])\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = plot_mat(df_corr, 'region_1_name', 'region_2_name', 'pearson_r', n_colors=200)\n",
    "bokeh.plotting.output_file('act_mat.html')\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(corr_mat, cmap=plt.cm.jet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the amygdala doing?\n",
    "Let's see which grayordinates comprise the amygdala.  To check, we look for any `region_name` that has `AMYGDALA` in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get indices associated with amygdala\n",
    "#inds = df['region_name'].str.contains(\"AMYGDALA\")\n",
    "inds_right = df['region_id'] == 6\n",
    "inds_left = df['region_id'] == 5\n",
    "\n",
    "# Count the number of unique grayordinates in there\n",
    "amyg_left = df[inds_left]\n",
    "amyg_right = df[inds_right]\n",
    "\n",
    "# Compute mean for each time point\n",
    "mean_amyg_left = amyg_left.groupby('time (s)')['voxel_value'].mean()\n",
    "mean_amyg_right = amyg_right.groupby('time (s)')['voxel_value'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.plot(t, mean_amyg_left)\n",
    "plt.plot(t, mean_amyg_right, color=sns.color_palette()[2])\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('mean voxel value')\n",
    "\n",
    "# Make interactive with Bokeh\n",
    "bokeh.plotting.show(bokeh.mpl.to_bokeh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['region_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.plot(mean_amyg_left, mean_amyg_right, marker='o', linestyle='None')\n",
    "plt.axis('equal')\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('mean voxel value')\n",
    "\n",
    "# Make interactive with Bokeh\n",
    "# bokeh.plotting.show(bokeh.mpl.to_bokeh())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have 647 grayordinates comprising the amygdala.  Let's plot the average activity of the amygdala over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute mean for each time point\n",
    "mean_amyg = df[inds].groupby('time (s)')['voxel_value'].mean()\n",
    "\n",
    "# Plot results\n",
    "plt.plot(t, mean_amyg)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('mean voxel value')\n",
    "\n",
    "# Make interactive with Bokeh\n",
    "bokeh.plotting.show(bokeh.mpl.to_bokeh())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should do more analysis, but, by eye, we see two different periodicities.  ??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
