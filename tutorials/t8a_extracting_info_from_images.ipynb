{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 8a: Extracting data from images\n",
    "\n",
    "*This tutorial was generated from a Jupyer notebook.  You can download the notebook [here](t8a_extracting_data_from_images.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Our numerical workhorses\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal\n",
    "import scipy.stats as st\n",
    "\n",
    "# BE/Bi 103 utilities\n",
    "import bebi103\n",
    "\n",
    "# Image processing tools\n",
    "import skimage\n",
    "import skimage.io\n",
    "\n",
    "# Import plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Magic function to make matplotlib inline; other style specs must come AFTER\n",
    "%matplotlib inline\n",
    "\n",
    "# This enables high res graphics inline (only use with static plots (non-Bokeh))\n",
    "# SVG is preferred, but there is a bug in Jupyter with vertical lines\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# JB's favorite Seaborn settings for notebooks\n",
    "rc = {'lines.linewidth': 2, \n",
    "      'axes.labelsize': 18, \n",
    "      'axes.titlesize': 18, \n",
    "      'axes.facecolor': 'DFDFE5'}\n",
    "sns.set_context('notebook', rc=rc)\n",
    "sns.set_style('darkgrid', rc=rc)\n",
    "\n",
    "# Import Bokeh modules for interactive plotting\n",
    "import bokeh.charts\n",
    "import bokeh.io\n",
    "import bokeh.plotting\n",
    "\n",
    "# Display graphics in this notebook\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "# Suppress future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start working with images, we need to know how to do all of the more mundane things with them, like organizing, loading, storing, etc.  Images that you want to analyze come is all sorts of formats and are organized in many different ways.  One instrument may give you a TIFF stack consisting of 8-bit images.  Another may give you a directory with lots of images in it with different names.  And so many instruments give you data in some proprietary format.  You need to know how to deal with lots of file formats and organization.\n",
    "\n",
    "In this course, we will analyze various images, including microscopy images.  I generally will give them to you in exactly the format I received them in.\n",
    "\n",
    "In today's tutorial, we will use a data set from some one of the students in the class (Mike Abrams) and two class alumni (Claire Bedbrook and Ravi Nath) in the class to practice loading images, defining ROIs, and some basic skills for extracting information from images.\n",
    "\n",
    "In this tutorial and throughout the class, we will use `scikit-image` to do almost all of our image processing.  It has continual development, with new features constantly being added.  For the fairly simple image processing we'll do in this course, it suffices for most of what we need.  The [course website](http://bebi103.caltech.edu/2015/links.html) has links to many other image processing toolkits, some Python-based and many not.  In particular, [OpenCV](http://opencv.org) is very powerful and has complete Python bindings, though not yet for the newest version 3.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set\n",
    "\n",
    "The data set comes from three Caltech graduate students, Mike Abrams, Claire Bedbrook, and Ravi Nath, who are working on studying day and night behavior in jellyfish.  They photograph jellyfish over time and study their pulsing behavior.  A typical image is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"jellyfish.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single jellyfish lives in each box.  The boxes are 4 by 4 inches.  The jellyfish constantly contract and relax in a pulsing motion.  Our task today is to get traces so we can measure pulse frequency.\n",
    "\n",
    "We have two different data sets, day and night, each contained in their own directory.  Each image they acquired is stored as a TIFF image.  The frame rate was 15 frames per second.\n",
    "\n",
    "Let's take a look at the structure of the directory with the images.  We will use the `glob` module, which allows us to specify string with wild cards to specify files of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The directory containing daytime data\n",
    "data_dir = '../data/abrams_et_al/Cassiopea_Pulsation/day'\n",
    "\n",
    "# Glob string for images\n",
    "im_glob = os.path.join(data_dir, '*.TIF')\n",
    "\n",
    "# Get list of files in directory\n",
    "im_list = glob.glob(im_glob)\n",
    "\n",
    "# Let's look at the first 10 entries\n",
    "im_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the images are nicely names so that successive frames are stored in order.  This is convenient, and when this is not the case, you should rename files so that when listed in alphabetical order, they also appear in temporal order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and displaying images\n",
    "The workhorse for much of our image processing is `scikit-image`.  We will load our first image using `scikit-image`.  When importing `scikit-image`, it is called `skimage`, and we need to import the submodules individually.  The submodule for reading and writing images is `skimage.io`.  We can use it to load and display an image.  Let's start with the first jellyfish image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the image using skimage\n",
    "im = skimage.io.imread(im_list[0])\n",
    "\n",
    "# Let's get information about the image\n",
    "print(type(im), im.dtype, im.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the image is stored as a NumPy array of 16-bit `int`s.  The array is 3$\\times$480$\\times$640, which means this is an RGB image.  The first index of the image indicates the channel (red, green, or blue).  Cameras often store black and white images as RGB, so let's see if all three channels are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test to see if each R, G, and B value is the same\n",
    "((im[0,:,:] == im[1,:,:]) &  (im[1,:,:] == im[2:,:])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed they are, so we can just consider one of the channels in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just slice the red channel\n",
    "im = im[0,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use `skimage.io.imshow()` to take a look at the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skimage.io.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the white gridlines (which we may or may not want), we can temporarily change Seaborn's axis style in a `with` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with sns.axes_style('white'):\n",
    "    skimage.io.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burning a scale bar\n",
    "\n",
    "Remember, scientific images are not just pretty pictures. It is crucially important that you know the interpixel distance and that the images have a scale bar when displayed, at least for publication.\n",
    "\n",
    "For some microscope set-ups, we already know the physical length corresponding to the interpixel distance.  We often take a picture of something with known dimension (such as a stage micrometer) to find the interpixel distance.  In the case of these jellyfish images, we know the boxes are 4$\\times$4 inches.  We can get the locations of the boxes in units of pixels using the `plt.ginput()` function.  This function records the coordinates of mouse clicks on a figure.  We can't really do this with graphics in the Jupyter notebook, but using a figure window, I get the following results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xy = [(346.43976683937819, 246.4948186528498),\n",
    "      (498.77137305699478, 251.93523316062181)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these points to compute the length of a side of a box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "box_length = np.sqrt((xy[1][1] - xy[0][1])**2 + (xy[1][0] - xy[0][0])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpixel distance is then 4 inches / `box_length`.  We will compute it in centimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interpixel_distance = 4 / box_length * 2.54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the interpixel distance, we can burn a scale bar.  To do this, we simply set a stripe of pixels in a black area of the image to be white (or vice versa).  The upper left corner of the image is black and makes for a nice place to put a scale bar.  We will burn a scale bar of 5 cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def burn_scale_bar(im, length, i_pos, j_pos, width=4, white=True):\n",
    "    \"\"\"\n",
    "    Burn a horizontal scale bar starting at i_pos, j_pos.\n",
    "    All units of inputs are interpixel distance.\n",
    "    \"\"\"\n",
    "    im_out = im.copy()\n",
    "    \n",
    "    if white:\n",
    "        pixel_val = skimage.dtype_limits(im)[1]\n",
    "    else:\n",
    "        pixel_val = skimage.dtype_limits(im)[0]\n",
    "    \n",
    "    im_out[i_pos-(width//2):i_pos+(width//2), j_pos:j_pos+length] = pixel_val\n",
    "    return im_out\n",
    "    \n",
    "# Put black scale bar\n",
    "im_sb = burn_scale_bar(im, 5 / interpixel_distance, 30, 10)\n",
    "\n",
    "# Look at it\n",
    "with sns.axes_style('white'):\n",
    "    skimage.io.imshow(im_sb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, though, that in the case of an image like this, we could just put something like, \"Jelly fish boxes are 4$\\times$4 inches,\" in our caption, and be done with it.  I'm showing you this because it is important to know how to do for many of the images you come across.\n",
    "\n",
    "`scikit-image` currently does not have capability to add text.  You can make scale bars with text easily when preparing figures for publication within a Python framework using [OpenCV](http://opencv.org/) or by using [Fiji](http://fiji.sc/).\n",
    "\n",
    "I want to make a final couple of notes on scale bars.  Some will argue that the scale bars could be added as vector graphics with nice, clean text.  There is no inherent problem with this, but in practice, images often get stretched and rearranged for display in presentations or in publications.  It is often difficult to control how various layers on objects will scale when this is done.  I prefer to keep myself out of trouble and burn in the scale bars.  Finally, make sure your scale bars are not burned in to the image you are actually analyzing, only those for viewing!  Remember, by burning in the scale bar in something you are analyzing, you are changing *data*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False coloring for image display\n",
    "\n",
    "Remember, showing an image on screen or printing it on a piece of paper is *plotting data*.  We are free to choose *how* we plot it.  The common convention is to show the image in gray scale, but we need not do that.  We can instead look at the image in false color.  (\"False\" is a misnomer here, since any color is just a representation of photon counts on a detector.)  *This is not adding or subtracting information; it is merely changing the way we display it.*  Since images are typically sequential data, sequential colormaps are preferred.  In cases where we might have stark separation between dark and light, a diverging colormap is useful.  My favorite sequential and diverging colormaps for images are, respectively, `plt.cm.viridis` (Matplotlib's fantastic new default colormap) and `plt.cm.RdBu_r`.  Let's look at the image with these colormaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up figure with subplots\n",
    "with sns.axes_style('white'):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax[0].imshow(im, cmap=plt.cm.viridis)\n",
    "    ax[1].imshow(im, cmap=plt.cm.RdBu_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often easier to see fine detail with false coloring.  While not important for our analysis, we can, for example, see the nuances in brightness in the sand on which the jellyfish sit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a stack of images\n",
    "While we have done some nice things with single images, we really want to be able to sequentially look at all of the images of the movie, a.k.a. the stack of images.\n",
    "\n",
    "`scikit-image` has a convenient way to do this using the `skimage.io.ImageCollection` class.  We simple give it a string matching the pattern of the file names we want to load.  In our case, this is `im_glob` that we have already defined.  An important kwarg is `conserve_memory`.  If there are many images, it is best to select this to be `True`.  For few images, selecting this to be `False` will result in better performance because all of the images will be read into RAM.  Let's load the daytime movie as an `ImageCollection`.  (Note: we're not actually loading them all in; we just creating an object that knows where the images are so we can access them at will.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ic = skimage.io.ImageCollection(im_glob, conserve_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Line profiler (you might not have this installed)\n",
    "%load_ext line_profiler\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import skimage.io\n",
    "import scipy.ndimage\n",
    "\n",
    "# The directory containing daytime data\n",
    "data_dir = '../data/abrams_et_al/Cassiopea_Pulsation/day'\n",
    "\n",
    "# Glob string for images\n",
    "im_glob = os.path.join(data_dir, '*.TIF')\n",
    "\n",
    "# Get list of files in directory\n",
    "im_list = glob.glob(im_glob)\n",
    "\n",
    "# Load image collection\n",
    "ic = skimage.io.ImageCollection(im_glob, conserve_memory=True)\n",
    "\n",
    "def load_40_images(ic, im_list, method):\n",
    "    ims = [None] * 40\n",
    "    if method == 'collection':\n",
    "        for i in range(40):\n",
    "            ims[i] = ic[i][0,:,:]\n",
    "    elif method == 'scipy':\n",
    "        for i in range(40):\n",
    "            ims[i] = scipy.ndimage.imread(im_list[i])\n",
    "    elif method == 'PIL':\n",
    "        for i in range(40):\n",
    "            im_PIL = Image.open(im_list[i])\n",
    "            ims[i] = np.array(im_PIL.getdata(), np.uint16)\n",
    "    elif method == 'skimage.io':\n",
    "        for i in range(40):\n",
    "            ims[i] = skimage.io.imread(im_list[i])[0,:,:]\n",
    "            \n",
    "print('ImageCollection:')\n",
    "%timeit load_40_images(ic, im_list, method='collection')\n",
    "\n",
    "print('\\nscipy:')\n",
    "%timeit load_40_images(ic, im_list, method='scipy')\n",
    "\n",
    "print('\\nPIL:')\n",
    "%timeit load_40_images(ic, im_list, method='PIL')\n",
    "\n",
    "print('\\nskimage.io:')\n",
    "%timeit load_40_images(ic, im_list, method='skimage.io')\n",
    "\n",
    "%lprun -T lp_results.txt -f gillespie_ssa gillespie_ssa(\\\n",
    "        params, simple_propensity, simple_update, population_0, time_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'lp_results.txt'. \n"
     ]
    }
   ],
   "source": [
    "# Line profiler (you might not have this installed)\n",
    "%load_ext line_profiler\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "\n",
    "# The directory containing daytime data\n",
    "data_dir = '../data/abrams_et_al/Cassiopea_Pulsation/day'\n",
    "\n",
    "# Glob string for images\n",
    "im_glob = os.path.join(data_dir, '*.TIF')\n",
    "\n",
    "# Get list of files in directory\n",
    "im_list = glob.glob(im_glob)\n",
    "\n",
    "def load_and_mean(im_list, n):\n",
    "    im_means = np.empty(n)\n",
    "    for i, fname in enumerate(im_list[:n]):\n",
    "        im = scipy.ndimage.imread(fname)\n",
    "        im_slice = im[300:500, 300:500]\n",
    "        im_means[i] = im_slice.mean()\n",
    "        \n",
    "    return im_means\n",
    "\n",
    "%lprun -T lp_results.txt -f load_and_mean load_and_mean(im_list, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ims = [None] * 4000\n",
    "for i in range(4000):\n",
    "    if i % 100 == 0:\n",
    "        print('i =', i, flush=True)\n",
    "    ims[i] = scipy.ndimage.imread(im_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skimage.io.imshow(ims[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, the `ImageCollection` has nice properties.  Typing `len(ic)` tells how many images are in the collection.  `ic[157]` is the 157th image in the collection.  There is one problem, though.  All of the images are read in with all three channels.  We would like to only include one channel, since the others are redundant.  To do this, we can instantiate the `ImageCollection` with the `load_func` kwarg.  This specifies a function that reads in and returns a NumPy array with the image.  The default is `load_func=skimage.io.imread`, but we can write out own.  We'll write a function that loads the image and then just returns the red channel and reinstantiate the `ImageCollection` using that load function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def squish_rgb(fname):\n",
    "    return skimage.io.imread(fname)[0,:,:]\n",
    "\n",
    "ic = skimage.io.ImageCollection(im_glob, conserve_memory=True, \n",
    "                                load_func=squish_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also have time stamps for the data in the images.  We know the frame rate is 15 frames per second, so we can attach times to each image.  Normally, these would be in the metadata of the images and we would fish that out, but for this example, we will just generate our own time points (in units of seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fps = 15\n",
    "t = np.arange(0, len(ic)) / fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting an ROI\n",
    "\n",
    "A **region of interest**, or ROI, is part of an image or image stack that we would like to study, ignoring the rest.  Depending on the images we are analyzing, we may be able to automatically detect ROIs based on well-defined criteria.  Often, though, we need to manually pick regions in an image as our ROI.\n",
    "\n",
    "`scikit-image` is an open source project that is constantly under development.  It currently does not have a way to specify ROIs, but it is on the list of functionality to be added.\n",
    "\n",
    "So, I wrote my own ROI utility, which is included in [`bebi103_utils`](https://github.com/justinbois/bebi103_utils).  It takes a set of vertices that define a polygon, the inside of which constitutes a region of interest.  (Note that the polygon cannot have crossing lines.)  It returns a tuple that contains a **mask** for the ROI, a **bounding box**, and the mask for an image consisting entirely of the bounding box.  An ROI mask is the same shape as the image, except where the image has a pixel value, the ROI mask has a `True` value if the pixel is within the ROI and has a `False` value it if is outside.  The bounding box defines the smallest square subimage that completely contains the ROI.  Finally, it is convenient to have a mask that is reindexed such that (0,0) is the upper left corner of the bounding box.\n",
    "\n",
    "To get the vertices, we can use the `plt.ginput()` function.  This cannot be demonstrated in an IPython notebook with inline `matplotlib`, so I do not show it here.  The syntax is below, with `ginput()` commented out, with its output instead hard coded.  In this example, I drew a square in the image to select a single box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select the condos as ROIs.  Use plt.ginput to get vertices\n",
    "# Uncomment the below to use ginput.  Hit enter when finished selecting verts\n",
    "# verts = plt.ginput(100)\n",
    "\n",
    "verts = [(348.79397748093913, 246.73039544685778),\n",
    "         (491.30157835613034, 249.91294732744518),\n",
    "         (487.05817584868049, 392.77416507825723),\n",
    "         (347.37950997845581, 386.76267819270322)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `bebi103.verts_to_roi()` to make make the ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roi, roi_bbox, roi_box = bebi103.verts_to_roi(verts, *ic[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the ROI, let's look at it.  We will use a trick where we take a grayscale image, convert it to RGB, and then add more blue in certain regions to highlight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make grayscale image that is now RGB\n",
    "im = np.dstack(3*[skimage.img_as_float(ic[0])])\n",
    "\n",
    "# Max out blue channel\n",
    "im[roi,2] = skimage.dtype_limits(im)[1]\n",
    "\n",
    "# Look at the image\n",
    "with sns.axes_style('white'):\n",
    "    skimage.io.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want to look at the region of the image that bounds the ROI, you can do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get cropped image and ROI within it\n",
    "im = ic[0][roi_bbox]\n",
    "im_cropped_roi = roi_box\n",
    "\n",
    "# Make grayscale image that is now RGB\n",
    "im = np.dstack(3*[skimage.img_as_float(im)])\n",
    "\n",
    "# Max out blue channel\n",
    "im[im_cropped_roi,2] = skimage.dtype_limits(im)[1]\n",
    "\n",
    "# Look at the image\n",
    "with sns.axes_style('white'):\n",
    "    skimage.io.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when working with ROIs, the `roi`, `roi_bbox`, and `roi_box` are useful tools to automatically index the original image to get what you are interested in.  We could also modify our `load_func` for our `ImageCollection` to just load in an ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define load_func that only gives region bounding ROI\n",
    "def load_roi(fname, roi_bbox=None):\n",
    "    if roi_bbox is None:\n",
    "        return skimage.io.imread(fname)[0,:,:]\n",
    "    else:\n",
    "        return skimage.io.imread(fname)[0,:,:][roi_bbox]\n",
    "\n",
    "# Load image collection\n",
    "ic = skimage.io.ImageCollection(im_glob, conserve_memory=True, \n",
    "                                load_func=load_roi, roi_bbox=roi_bbox)\n",
    "\n",
    "# Look at first image\n",
    "with sns.axes_style('white'):\n",
    "    skimage.io.imshow(ic[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some simple analysis\n",
    "\n",
    "We are interested in the rate at which the jellyfish pulse.  Since the jellyfish are dark on a black background, we could just watch how the total pixel intensity of our respective ROIs change over time to get the pulsing frequency.  This will not really tell us about the shape of the jellyfish or any fine detail, but it will hopefully be enough to get us a rough estimate of pulsing frequency.\n",
    "\n",
    "To be more concrete, our goal is to find the distribution of inter-contraction times.  If the distribution is tightly peaked, we have periodic pulsing and we can estimate a frequency.  Otherwise, we might notice pause events that we can see qualitatively in the movies.\n",
    "\n",
    "We will analyze the jellyfish in our current ROI.  To start with, we'll just compute the total pixel intensity through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up NumPy array to store total pixel intensity\n",
    "total_int = np.empty(len(t))\n",
    "\n",
    "# Look through and compute total intensity in the ROI\n",
    "for i, im in enumerate(ic):\n",
    "    total_int[i] = ic[i][roi_box].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(t, total_int)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('total intensity')\n",
    "plt.margins(0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the intensity units are arbitrary, we can let's subtract the mean and rescale the data so they go from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_int -= total_int.mean()\n",
    "total_int = 1 + 2 / (total_int.max() - total_int.min()) \\\n",
    "                * (total_int - total_int.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be easier to inspect these data by looking at a smaller time window.  We'll look at the 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tinds = np.s_[:fps*30]\n",
    "plt.plot(t[tinds], total_int[tinds])\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('normalized intensity')\n",
    "plt.margins(0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better yet, we can just plot it with Bokeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'time (s)': t, 'normalized intensity': total_int})\n",
    "p = bokeh.charts.Line(df, x='time (s)', y='normalized intensity', \n",
    "                      color='dodgerblue')\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The periodogram\n",
    "This looks quite periodic, so let's have a look at the periodogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine frequencies\n",
    "f = np.fft.fftfreq(len(t)) * fps\n",
    "\n",
    "# Compute power spectral density\n",
    "psd = np.abs(np.fft.fft(total_int))**2 / len(t)\n",
    "\n",
    "# Make plot\n",
    "plt.plot(f[f>0], psd[f>0])\n",
    "plt.xlabel('freq (Hz)')\n",
    "plt.ylabel('PSD')\n",
    "plt.margins(0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a very strong peak near 1 Hz, which is the typical pulse frequency.  We could have guessed this just by looking at the plots.  Nonetheless, there are other frequencies present, most strikingly at about 0.4 Hz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying peaks\n",
    "Now let's identify the peaks in the time trace.  Our strategy is as follows.\n",
    "\n",
    "1. Indentify peak regions as between positive and negative crossings of zero.\n",
    "2. Localize the maximum of the peak as the maximum between upward and downward crossings of zero.\n",
    "3. Get sub-sampling accuracy for peak placement by fitting a quadratic to the maximum point and its nearest neighbors, like we did in [Tutorial 7a](t7a_time_series.html).\n",
    "\n",
    "We first write a quick function to use the quadratic fitting method to get a local maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def local_min(x, y):\n",
    "    \"\"\"\n",
    "    Fit three points to quadratic and return location of maximum or minimum.\n",
    "    \"\"\"\n",
    "    a, b, c = np.polyfit(x, y, 2)\n",
    "    x_max = -b / 2 / a\n",
    "    return x_max, a*x_max**2 + b*x_max + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll find the positions of maxima to the accuracy of our sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find up and down crossing indices\n",
    "up_crossing_inds = np.where(\n",
    "    np.logical_and(total_int[:-1] < 0, total_int[1:] >= 0))[0]\n",
    "down_crossing_inds = np.where(\n",
    "    np.logical_and(total_int[:-1] > 0, total_int[1:] <= 0))[0] + 1\n",
    "\n",
    "# Make sure upcrossing are first\n",
    "if down_crossing_inds[0] < up_crossing_inds[0]:\n",
    "    down_crossing_inds = down_crossing_inds[1:]\n",
    "    \n",
    "# Make sure downcrossing last\n",
    "if up_crossing_inds[-1] > down_crossing_inds[-1]:\n",
    "    up_crossing_inds = up_crossing_inds[:-1]\n",
    "    \n",
    "# Find maxima\n",
    "t_peaks = np.empty(len(up_crossing_inds))\n",
    "peaks = np.empty_like(t_peaks)\n",
    "for i, uc in enumerate(up_crossing_inds):\n",
    "    ind = uc + np.argmax(total_int[uc:down_crossing_inds[i]+1])\n",
    "    t_peaks[i], peaks[i] = local_min(t[ind-1:ind+2], total_int[ind-1:ind+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we did my making a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the figure (this is like a canvas you will paint on)\n",
    "p = bokeh.plotting.figure(background_fill='#DFDFE5', plot_width=650, \n",
    "                          plot_height=450)\n",
    "p.xgrid.grid_line_color = 'white'\n",
    "p.ygrid.grid_line_color = 'white'\n",
    "p.xaxis.axis_label = 'time (s)'\n",
    "p.yaxis.axis_label = 'normalized intensity'\n",
    "\n",
    "# Specify the glyphs\n",
    "p.line(t, total_int, color='dodgerblue')\n",
    "p.circle(t_peaks, peaks, size=5, color='tomato')\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we did a good job finding the peaks.  Now, let's compute the inter-contraction times and see how they change over the course of the reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Intercontractions times\n",
    "t_int = np.diff(t_peaks)\n",
    "\n",
    "# Plot them!\n",
    "plt.plot(t_int)\n",
    "plt.xlabel('contraction number')\n",
    "plt.ylabel('intercontraction time (s)')\n",
    "plt.margins(0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get occasional pauses in contractions with an average contraction rate of about 1.05 Hz, as we expected from the periodogram.  Apparently, the jellyfish starting contracting faster between contractions 120 and 150.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of inter-contraction times\n",
    "Let's now compute and plot the ECDF of inter-contraction times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make x and y values for ECDF\n",
    "x = np.sort(t_int)\n",
    "y = np.arange(1, len(x)+1) / len(x)\n",
    "plt.plot(x, y, '.')\n",
    "plt.xlabel('intercontraction time (s)')\n",
    "plt.ylabel('ECDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tails of the ECDF are shallower than a Gaussian, which implies that there are pausing events and also faster contractions.  We can see this more concretely with a Q-Q plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute mean and standard deviation based on middle 50th percentile\n",
    "mu = x.mean()\n",
    "sigma = x.std()\n",
    "\n",
    "# Make draws\n",
    "theor_x = np.array(\n",
    "    [np.sort(st.norm.rvs(mu, sigma, size=len(x))) for _ in range(1000)])\n",
    "\n",
    "# Upper and lower bounds\n",
    "low_theor, up_theor = np.percentile(theor_x, (2.5, 97.5), axis=0)\n",
    "\n",
    "# Plot Q-Q plots with 95% conf.\n",
    "plt.fill_between(x, up_theor, low_theor, alpha=0.7)\n",
    "\n",
    "# Plot 45 degree line\n",
    "x_lim = plt.gca().get_xlim()\n",
    "plt.plot(x_lim, x_lim, 'k-')\n",
    "\n",
    "# Tidy up\n",
    "plt.margins(0.02)\n",
    "plt.xlabel('time between contractions (s)')\n",
    "plt.ylabel('time between contractions (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-Gaussianity is more pronounced at the high end of the inter-contraction time, indicative of occasional pauses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Can you do the similar analysis with the night time data set and compare the results to the day?  Can you do it for all eight jellyfish?  What do you observe?  This is actually one of your homework problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
