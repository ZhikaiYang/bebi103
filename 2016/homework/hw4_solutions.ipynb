{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BE/Bi 103, Fall 2016: Homework 4\n",
    "## Due 1pm, Sunday, October 23\n",
    "\n",
    "(c) 2016 Justin Bois. With the exception of the images, this work is licensed under a [Creative Commons Attribution License CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/). All code contained therein is licensed under an [MIT license](https://opensource.org/licenses/MIT).\n",
    "\n",
    "*This homework was generated from an Jupyter notebook.  You can download the notebook [here](hw4.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"50ca0194-68bd-4b27-8373-b99cdf07f2cb\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      Bokeh.$(\"#50ca0194-68bd-4b27-8373-b99cdf07f2cb\").text(\"BokehJS successfully loaded.\");\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"50ca0194-68bd-4b27-8373-b99cdf07f2cb\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '50ca0194-68bd-4b27-8373-b99cdf07f2cb' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#50ca0194-68bd-4b27-8373-b99cdf07f2cb\").text(\"BokehJS is loading...\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === \"1\") {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (!force) {\n",
       "      var cell = $(\"#50ca0194-68bd-4b27-8373-b99cdf07f2cb\").parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "import bokeh.plotting\n",
    "import bokeh.layouts\n",
    "import bokeh.io\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.1: Writing you own MCMC sampler (60 pts + 40 pts extra credit)\n",
    "\n",
    "**a)** Write your own MCMC sampler that employs a Metropolis-Hastings algorithm that uses a Gaussian proposal distribution. Since you are sampling multiple parameters, your proposal distribution will be multi-dimensional. You can use a Gaussian proposal distribution with a diagonal covariance. In other words, you generate a proposal for each variable in the posterior independently.\n",
    "\n",
    "You can organize your code how you like, but here is a suggestion.\n",
    "\n",
    "* Write a function that takes (or rejects) a Metropolis-Hastings step. It should look something like the below (obviously where it does something instead of `pass`ing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mh_step(x, log_post, logpost_current, sigma, args=()):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (n_variables,)\n",
    "        The present location of the walker in parameter space.\n",
    "    log_post : function\n",
    "        The function to compute the log posterior. It has call\n",
    "        signature `log_post(x, *args)`.\n",
    "    log_post_current : float\n",
    "        The current value of the log posterior.\n",
    "    sigma : ndarray, shape (n_variables, )\n",
    "        The standard deviations for the proposal distribution.\n",
    "    args : tuple\n",
    "        Additional arguments passed to `log_post()` function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : ndarray, shape (n_variables,)\n",
    "        The position of the walker after the Metropolis-Hastings\n",
    "        step. If no step is taken, returns the inputted `x`.\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write another function that calls that function over and over again to do the sampling. It should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mh_sample(x0, log_post, sigma=None, args=(), n_burn=1000, n_steps=1000,\n",
    "              variable_names=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x0 : ndarray, shape (n_variables,)\n",
    "        The starting location of a walker in parameter space.\n",
    "    log_post : function\n",
    "        The function to compute the log posterior. It has call\n",
    "        signature `log_post(x, *args)`.\n",
    "    sigma : ndarray, shape (n_variables, )\n",
    "        The standard deviations for the proposal distribution.\n",
    "        If None, takes all values to be one.\n",
    "    args : tuple\n",
    "        Additional arguments passed to `log_post()` function.\n",
    "    n_burn : int, default 1000\n",
    "        Number of burn-in steps.\n",
    "    n_steps : int, default 1000\n",
    "        Number of steps to take after burn-in.\n",
    "    variable_names : list, length n_variables\n",
    "        List of names of variables. If None, then variable names\n",
    "        are sequential integers.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : DataFrame\n",
    "        The first `n_variables` columns contain the samples.\n",
    "        Additionally, column 'lnprob' has the log posterior value\n",
    "        at each sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** To test your code, we will get samples out of a known distribution. We will use a bivariate Gaussian with a mean of $\\boldsymbol{\\mu} = (10, 20)$ and covariance matrix of \n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\sigma} = \\begin{pmatrix}\n",
    "4 & -2 \\\\\n",
    "-2 & 6\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "I have written the function to be unnormalized and JITted with numba for optimal speed.\n",
    "\n",
    "Do not be confused: In this test function we are sampling $\\mathbf{x}$. This is not sampling a posterior; it's just a test for your code. You will pass `log_test_distribution` as the `log_post` argument in the above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = np.array([10.0, 20])\n",
    "cov = np.array([[4, -2],[-2, 6]])\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def log_test_distribution(x, mu, inv_cov):\n",
    "    \"\"\"\n",
    "    Unnormalized log posterior of a multivariate Gaussian.\n",
    "    \"\"\"\n",
    "    return -np.dot((x-mu), np.dot(inv_cov, (x-mu))) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compute the means and covariance (using `np.cov()`) of your samples, you should come close to the inputed means and covariance. You might also want to plot your samples using `corner.corner()` to make sure everything makes sense.\n",
    "\n",
    "**c)** (20 pts extra credit) Add in some logic to your Metropolis-Hastings sampler to enable *tuning*. This is the process of automatically adjusting the $\\sigma$ in the proposal distribution such that the acceptance rate is desirable. The target acceptance rate is about 0.4. The developers of [PyMC3](https://github.com/pymc-devs/pymc3) use the scheme below, which is reasonable.\n",
    "\n",
    "|Acceptance rate|Standard deviation adaptation|\n",
    "|:---:|:-------------------:|\n",
    "| < 0.001        |$\\times$ 0.1|\n",
    "|< 0.05         |$\\times$ 0.5|\n",
    "|< 0.2          |$\\times$ 0.9|\n",
    "|> 0.5          |$\\times$ 1.1|\n",
    "|> 0.75         |$\\times$ 2|\n",
    "|> 0.95         |$\\times$ 10|\n",
    "\n",
    "Be sure to test your code to demonstrate that it works.\n",
    "\n",
    "**d)** (20 pts extra credit) Either adapt the functions you already wrote or write new ones to enable sampling of discrete variables. Again, be sure to test your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.1: solution\n",
    "\n",
    "I will complete part (a) and parts (c) and (d) together in defining my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def adjust_sigma(acc_rate, sigma):\n",
    "    if acc_rate < 0.001:\n",
    "        return sigma * 0.1\n",
    "    elif acc_rate < 0.05:\n",
    "        return sigma * 0.5\n",
    "    elif acc_rate < 0.2:\n",
    "        return sigma * 0.9\n",
    "    elif acc_rate > 0.95:\n",
    "        return sigma * 10.0\n",
    "    elif acc_rate > 0.75:\n",
    "        return sigma * 2.0\n",
    "    elif acc_rate > 0.5:\n",
    "        return sigma * 1.1\n",
    "    else:\n",
    "        return sigma\n",
    "    \n",
    "def mh_sample(logpost, x0, sigma0, n_burn, n_samples, args=(), \n",
    "              tune_interval=100, quiet=False):\n",
    "    \"\"\"\n",
    "    Perform Metropolis sampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Never tune if asked not to\n",
    "    if tune_interval <= 0:\n",
    "        tune_interval = n_burn + n_samples\n",
    "    elif tune_interval < 50:\n",
    "        raise RuntimeError('Tune interval should be at least 50.')\n",
    "    \n",
    "    @numba.jit(nopython=True)\n",
    "    def mh_step_jitted(x, logpost_current, sigma, args=()):\n",
    "        \"\"\"\n",
    "        JITted version\n",
    "        \"\"\"\n",
    "        # Draw the next step\n",
    "        x_next = np.empty(len(x), dtype=x.dtype)\n",
    "        for i in range(len(x)):\n",
    "            x_next[i] = np.random.normal(x[i], sigma[i])\n",
    "\n",
    "        # Compute log posterior\n",
    "        logpost_new = logpost(x_next, *args)\n",
    "\n",
    "        # Compute the log Metropolis ratio\n",
    "        log_r = logpost_new - logpost_current\n",
    "\n",
    "        # Accept or reject step\n",
    "        if log_r >= 0 or np.random.random() < np.exp(log_r):\n",
    "            return x_next, logpost_new, True\n",
    "        else:\n",
    "            return x, logpost_current, False\n",
    "\n",
    "        \n",
    "    def mh_step(x, logpost_current, sigma, args=()):\n",
    "        \"\"\"\n",
    "        Non-jitted version\n",
    "        \"\"\"\n",
    "        # Draw the next step\n",
    "        x_next = np.empty(len(x), dtype=x.dtype)\n",
    "        for i in range(len(x)):\n",
    "            x_next[i] = np.random.normal(x[i], sigma[i])\n",
    "    \n",
    "        # Compute log posterior\n",
    "        logpost_new = logpost(x_next, *args)\n",
    "\n",
    "        # Compute the log Metropolis ratio\n",
    "        log_r = logpost_new - logpost_current\n",
    "\n",
    "        # Accept or reject step\n",
    "        if log_r >= 0 or np.random.random() < np.exp(log_r):\n",
    "            return x_next, logpost_new, True\n",
    "        else:\n",
    "            return x, logpost_current, False\n",
    "\n",
    "    @numba.jit(nopython=True)\n",
    "    def sample_jitted(x0, sigma0, n_burn, n_samples, args=()):\n",
    "        # Initialize\n",
    "        n_accept = 0\n",
    "        n_accept_total = 0\n",
    "        n_steps = 0\n",
    "        n_tune_steps = 0\n",
    "        x = np.copy(x0)\n",
    "        sigma = sigma0\n",
    "        logprob = np.empty(n_samples)\n",
    "        logpost_current = logpost(x, *args)\n",
    "\n",
    "        # Burn in\n",
    "        while n_steps < n_burn:\n",
    "            while n_tune_steps < tune_interval and n_steps < n_burn:\n",
    "                x, logpost_current, accept = mh_step_jitted(\n",
    "                                            x, logpost_current, sigma, args)\n",
    "                n_steps += 1\n",
    "                n_tune_steps += 1\n",
    "                n_accept += accept\n",
    "            sigma = adjust_sigma(n_accept/tune_interval, sigma)\n",
    "            n_accept = 0\n",
    "            n_tune_steps = 0\n",
    "\n",
    "        # Samples\n",
    "        x_samples = np.empty((n_samples, len(x)))\n",
    "        n_steps = 0\n",
    "        while n_steps < n_samples:\n",
    "            while n_tune_steps < tune_interval and n_steps < n_samples:\n",
    "                x, logpost_current, accept = mh_step_jitted(\n",
    "                                            x, logpost_current, sigma, args)\n",
    "                x_samples[n_steps,:] = x\n",
    "                logprop[n_steps] = logpost_current\n",
    "                n_tune_steps += 1\n",
    "                n_accept += accept\n",
    "                n_accept_total += accept\n",
    "                n_steps += 1\n",
    "            sigma = adjust_sigma(n_accept/tune_interval, sigma)\n",
    "            n_accept = 0\n",
    "            n_tune_steps = 0\n",
    "                \n",
    "        return x_samples, n_accept_total / n_samples\n",
    "\n",
    "\n",
    "    def sample(x0, sigma0, n_burn, n_samples, args=()):\n",
    "        # Initialize\n",
    "        n_accept = 0\n",
    "        n_accept_total = 0\n",
    "        n_steps = 0\n",
    "        n_tune_steps = 0\n",
    "        x = np.copy(x0)\n",
    "        sigma = sigma0\n",
    "        logpost_current = logpost(x, *args)\n",
    "\n",
    "        # Burn in\n",
    "        while n_steps < n_burn:\n",
    "            while n_tune_steps < tune_interval and n_steps < n_burn:\n",
    "                x, logpost_current, accept = mh_step_jitted(\n",
    "                                            x, logpost_current, sigma, args)\n",
    "                n_steps += 1\n",
    "                n_tune_steps += 1\n",
    "                n_accept += accept\n",
    "            sigma = adjust_sigma(n_accept/tune_interval, sigma)\n",
    "            n_accept = 0\n",
    "            n_tune_steps = 0\n",
    "\n",
    "        # Samples\n",
    "        x_samples = np.empty((n_samples, len(x)))\n",
    "        n_steps = 0\n",
    "        while n_steps < n_samples:\n",
    "            x, logpost_current, accept = mh_step_jitted(\n",
    "                                            x, logpost_current, sigma, args)\n",
    "            x_samples[n_steps,:] = x\n",
    "            n_steps += 1\n",
    "            n_accept_total += accept\n",
    "\n",
    "        return x_samples, n_accept_total / n_samples\n",
    "\n",
    "    try:\n",
    "        return sample_jitted(x0, sigma0, n_burn, n_samples, args)\n",
    "    except:\n",
    "        if not quiet:\n",
    "            print('Log posterior not properly jitted. Calculation may be slow.',\n",
    "                 flush=True)\n",
    "        return sample(x0, sigma0, n_burn, n_samples, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x0 = np.array([10.0, 20.0])\n",
    "sigma0 = np.array([1.0, 1.0])\n",
    "n_burn = 500000\n",
    "n_samples = 500000\n",
    "args = (mu, inv_cov)\n",
    "samples, acc_frac = mh_sample(log_test_distribution, x0, sigma0, n_burn, n_samples, args=args, \n",
    "              quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.347082"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 3.9983074, -1.9950861],\n",
       "        [-1.9950861,  5.9855583]]), array([  9.99151754,  20.00926679]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(samples[:,0], samples[:,1]), np.mean(samples, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.2: MCMC with Boolean data (40 pts)\n",
    "\n",
    "In this problem, we will work with data of the True/False type. Lots of data sets in the biological sciences are like this. For example, we might look at a certain mutation in *Drosophila* that affects development and we might check whether or not eggs hatch.\n",
    "\n",
    "The data we will use comes from an experiment we did the last couple years in [Bi 1x](http://bi1x.caltech.edu/) here at Caltech. The experiment was developed by Meaghan Sullivan. We studies a neural circuit in *C. elegans* using optogenetics.\n",
    "\n",
    "A neural circuit is a series of interconnected neurons that create a pathway to transmit a signal from where it is received, to where it causes a behavioral response in an animal.  An example is the neural circuit involved in reversals in *C. elegans*.  This circuit consists of three types of neurons: sensory neurons receive stimuli from the environment, command interneurons integrate information from many sensory neurons and pass a signal to the motor neurons, and motor neurons control worm behavior, such as reversals.\n",
    "\n",
    "There are six neurons acting in a circuit that responds to environmental cues and triggers a reversal, a shown in the figure below (based on [Schultheis et. al. 2011](10.1371/journal.pone.0018766)).  These include four sensory neurons (ALM, AVM, ASH, and PLM).  Each sensory neuron is sensitive to a different type of stimulus.  For example, the sensory neuron we are studying (ASH) is sensitive to chemosensory stimuli such as toxins, while another neuron (PLM) is sensitive to mechanical stimuli (touch) in the posterior part of the worm's body.  The sensory neurons send signals that are integrated by two command interneurons (AVA and AVD).  Each sensory neuron can provide an impulse to the command interneurons at any time.  In order for the command interneuron to fire and activate motor neurons, the sum of the stimuli at any point in time must exceed a certain threshold.  Once the stimuli from one or more sensory neurons has induced an action potential in a command interneuron, that signal is passed to motor neurons which will modulate worm behavior.\n",
    "\n",
    "![Reversal neural network](reversal_neural_network.png)\n",
    "\n",
    "In the experiment, we used optogenetics to dissect the function of individual neurons in this circuit.  We worked with two optogenetic worm strains.  The ASH strain has channelrhodopsin (ChR2, represented by a red barrel in the figure above) expressed only in the ASH sensory neuron.  When we shine blue light on this strain, we should activate the ChR2, which will allow sodium and calcium cations to flow into the neuron, simulating an action potential. We want to quantify how robustly this stimulation will cause the worm to exhibit aversion behavior and reverse.  \n",
    "\n",
    "We also studied an AVA strain that has channelrhodopsin expressed only in the AVA command interneuron.  Our goal is to quantify the effects of stimulating this neuron in terms of reversals compared to the ASH neuron and to wild type.\n",
    "\n",
    "The True/False data here are the whether or not the worms undergo a reversal. Here is what the students observed.\n",
    "\n",
    "|Strain|Year|Trials|Reversals|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|WT|2016|36|6|\n",
    "|ASH|2016|35|12|\n",
    "|AVA|2016|36|30|\n",
    "|WT|2015|35|0|\n",
    "|ASH|2015|35|9|\n",
    "|AVA|2015|36|33|\n",
    "\n",
    "For the purposes of this problem, assume that we can pool the results from the two years to have 6/71 reversals for wild type, 21/70 reversals for ASH, and 63/72 reversals for AVA.\n",
    "\n",
    "Our goal is to estimate $p$, the probability of reversal for each strain. That is to say, we want to compute $P(p\\mid r, n, I)$, where $r$ is the number of reversals in $n$ trials.\n",
    "\n",
    "**a)** Write down an expression for the posterior distribution. That is, write down an expression for the likelihood and prior. You can actually compute the evidence analytically, but that is not necessary for this problem; you can just write down the likelihood and prior.\n",
    "\n",
    "**b)** Use your Metropolis-Hastings sampler to sample this posterior for $p$ for each of the three strains. If you were unable to compute problem 3.1, you may use `emcee` to do the sampling. Plot the results. Do you think there is a difference between the wild type and ASH? How about between ASH and AVA?\n",
    "\n",
    "**c)** The posterior plots from part **(b)** are illuminating, but suppose we want to quantify *the difference* in reversal probability between the two strains, say strain 1 and strain 2.  That is, we want to compute $P(\\delta\\mid D, I)$, where $\\delta \\equiv p_2 - p_1$. Use MCMC to compute this, employing your Metropolis-Hastings solver if you can, using `emcee` otherwise.\n",
    "\n",
    "After you have done that with MCMC, it is useful to look at what a pain it is to attempt it analytically. I am just showing this as a lesson to you. To compute the distribution $P(\\delta\\mid D, I)$, we use the fact that we can easily compute the joint probability distribution because the two strains are independent.  To ease notation, we will note that the prior $P(p_1, p_2\\mid I)$ is only nonzero if $0\\le p_1,p_2 \\le 1$, instead of writing it explicitly.  The posterior is\n",
    "\n",
    "\\begin{align}\n",
    "P(p_1, p_2\\mid D, I) &= \n",
    "\\frac{(n_1+1)!\\,(n_2+1)!}{(n_1-n_{r1})!\\,n_{r1}!\\,(n_2 - n_{r2})!\\,n_{r2}!} \\\\[0.5em]\n",
    "&\\;\\;\\;\\;\\;\\;\\;\\;\\times\\,p_1^{n_{r1}}\\,(1-p_1)^{n_1-n_{r1}}\\,p_2^{n_{r2}}\\,(1-p_2)^{n_2-n_{r2}}.\n",
    "\\end{align}\n",
    "\n",
    "We can define new variables $\\delta = p_2 - p_1$ and $\\gamma = p_1 + p_2$.  Then, we have $p_1 = (\\gamma - \\delta) / 2$ and $p_2 = (\\gamma + \\delta) / 2$. \n",
    "Again, to ease notation, we note that $P(\\gamma, \\delta\\mid D, I)$ is nonzero only when $-1 \\le \\delta \\le 1$ and $|\\delta| \\le \\gamma \\le 2 - |\\delta|$.  By the change of variables formula for probability distributions we have\n",
    "\n",
    "\\begin{align}\n",
    "P(\\gamma, \\delta \\mid D, I) =\n",
    "\\begin{vmatrix}\n",
    "\\mathrm{d}p_1/\\mathrm{d}\\gamma & \\mathrm{d}p_1/\\mathrm{d}\\delta \\\\\n",
    "\\mathrm{d}p_2/\\mathrm{d}\\gamma & \\mathrm{d}p_2/\\mathrm{d}\\delta\n",
    "\\end{vmatrix}\n",
    "P(p_1, p_2 \\mid n_{r1}, n_{r2}, n_1, n_2, I)\n",
    "= \\frac{1}{2}\\,P(p_1, p_2 \\mid D, I).\n",
    "\\end{align}\n",
    "\n",
    "Thus, we have\n",
    "\n",
    "\\begin{align}\n",
    "P(\\gamma, \\delta\\mid D, I) &= \\frac{(n_1+1)!\\,(n_2+1)!}{2(n_1-n_{r1})!\\,(n_2-n_{r2})!\\,n_1!\\,n_2!} \\\\\n",
    "&\\;\\;\\;\\;\\times \n",
    "\\left(\\frac{\\gamma-\\delta}{2}\\right)^{n_{r1}}\\,\\left(1-\\frac{\\gamma-\\delta}{2}\\right)^{n_1-n_{r1}} \\left(\\frac{\\gamma+\\delta}{2}\\right)^{n_{r2}}\\,\\left(1-\\frac{\\gamma+\\delta}{2}\\right)^{n_2-n_{r2}}.\n",
    "\\end{align}\n",
    "\n",
    "Finally, to find $P(\\delta\\mid D, I)$, we marginalize by integrating over all possible values of $\\gamma$.\n",
    "\n",
    "\\begin{align}\n",
    "P(\\delta\\mid D, I) =\n",
    "\\int_{|\\delta|}^{2-|\\delta|}\\mathrm{d}\\gamma\\, P(\\gamma, \\delta\\mid D, I)\n",
    "\\end{align}\n",
    "\n",
    "We can expand each of the multiplied terms in the integrand into a polynomial using the binomial theorem, and can then multiply the polynomials together to get a polynomial expression for the integrand.  This can then be integrated.  There is a technical term describing this process: *a big mess*. Ouch. *This* is a major motivation for using MCMC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
