In the last lecture, we learned about Bayes's theorem as a way to
update a hypothesis in light of new data.  We use the word
``hypothesis'' very loosely here.  Remember, in the Bayesian view,
probability can describe the plausibility of any proposition.  The
value of a parameter is such a proposition.  In this lecture, we will
learn about how to do a Bayesian estimate of a parameter.


% %%%%%%%%%%%%%%%%%%%
\subsection{Bayes's theorem as applied to simple parameter estimation}
We will consider one of the simplest examples of parameter estimation.
Let's say we measure a parameter $\mu$ in multiple independent
experiments.  This could be beak depths of finches, fluorescence
intensity on a cell, dissociation constant for two bound proteins,
etc.  The possibilities abound.  You can have whatever your favorite
measurement is in mind during this analysis.

Our measurements of this parameter are $D = \{x_1, x_2, \ldots x_n\}$.
Our ``hypothesis'' in this case, is the value of the parameter $\mu$.
So, we wish to calculate $P(\mu \mid D, I)$, the posterior probability
distribution for the parameter $\mu$, given the data.  Values of $\mu$
for which the posterior probability is high are more probable (that
is, more plausible) than those for which is it low.

To compute the posterior probability, we use Bayes's theorem.
\begin{align}
P(\mu\mid D, I) = \frac{P(D\mid \mu, I)\,P(\mu \mid I)}{P(D\mid I)}.
\end{align}
Since the evidence, $P(D\mid I)$ does not depend on the parameter of
interest, $\mu$, it is really just a normalization constant, so we do
not need to consider it explicitly.  We now have to specify the
likelihood $P(D\mid \mu, I)$ and the prior $P(\mu \mid I)$.


% %%%%%%%%%%%%%%%%%%%
\subsection{The likelihood}
To specify the likelihood, we have to ask what we expect from the
data, given a value of $\mu$.  If there are no errors or confounding
factors at all in our measurements, we expect $x_i = \mu$ for all $i$.
In this case
\begin{align}
P(D\mid \mu, I) = \prod_{i\in D}\delta(x_i - \mu),
\end{align}
the product of Dirac delta functions.  Of course, this is really never
the case.  There will be some errors in measurement and/or the system
has variables that confound the measurement.  What, then should we
choose for our likelihood?

This question is made sharper if we think about the likelihood in
terms of the \textit{statistical model} we defined in the last lecture.  It is the
probability distribution that describes how the data relate to the
parameter we are trying to measure. Indeed, specifying the likelihood is part of the modeling process. In \href{http://bebi103.caltech.edu/2016/tutorials/t3a_probability_distributions.html}{Tutorial 3a}, we will
learn more about probability distributions, but for now, we will
introduce one useful distribution to use in our analyses.

% %%%%%%%%%%%%%%%%%%%
\subsection{The Gaussian distribution}
A Gaussian, or Normal, probability distribution has a probability
density function (PDF) of
\begin{align}
  P(x \mid \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\,
\exp\left\{-\frac{(x - \mu)^2}{2\sigma^2}\right\}.
\end{align}
The parameter $\mu$ is called the mean of the distribution and
$\sigma^2$ is the variance, with $\sigma$ being called the standard
deviation. Importantly, the mean and standard deviation in this context are \textit{names of parameters} of the distribution; they are not what you compute directly from data.

The \textbf{central limit theorem} says that any quantity that emerges
from a large number of subprocesses tends to be Gaussian distributed,
provided none of the subprocesses is very broadly distributed.  We
will not prove this important theorem, but we will make use of it when
choosing likelihood distributions.

Indeed, in the simple case of estimating a single parameter where many
processes may contribute to noise in the measurement, the Gaussian
distribution is a good choice for a likelihood.\footnote{It is also the
  \textbf{maximal entropy distribution} for a system with well-defined
  first and second moments, but we will not talk about entropy in this
  class.}

More generally, the multi-dimensional Gaussian distribution for
$\mathbf{x} = (x_1, x_2, \cdots, x_n)$ is
\begin{align}
  P(\mathbf{x} \mid \mu, \boldsymbol{\sigma}) = (2\pi)^{-\frac{n}{2}} \left(\det \boldsymbol{\sigma}^2\right)^{-\frac{1}{2}}\,
  \exp\left\{-\frac{1}{2}(\mathbf{x} - \mu)^T\cdot (\boldsymbol{\sigma}^2)^{-1}\cdot(\mathbf{x} - \mu)\right\},
\end{align}
where $\boldsymbol{\sigma}^2$ is a symmetric positive definite matrix
called the \textbf{covariance matrix}.  If off-diagonal entry
$(\boldsymbol{\sigma}^2)_{ij}$ is nonzero, then $x_i$ and $x_j$ are
correlated.  In the case where all $x_i$ are independent, all
off-diagonal terms in the covariance matrix are zero, and the
multidimensional Gaussian distribution reduces to
\begin{align}
  P(\mathbf{x} \mid \mu, \boldsymbol{\sigma}) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma_i^2}}\,
  \exp\left\{-\frac{(x_i - \mu)^2}{2\sigma_i^2}\right\},
\end{align}
where $\sigma^2_i$ is the $i$th entry along the diagonal of the
covariance matrix.  This is the variance associated with measurement
$i$.  So, if all independent measurements have the same variance, the
multi-dimensional Gaussian reduces to
\begin{align}
P(\mathbf{x} \mid \mu, \sigma) = \left(\frac{1}{2\pi \sigma^2} \right)^{-\frac{n}{2}}\,
  \exp\left\{-\frac{1}{2\sigma^2}\,\sum_{i=1}^n (x_i - \mu)^2\right\}.
\end{align}



% %%%%%%%%%%%%%%%%%%%
\subsection{The likelihood revisited: and another parameter}
The Gaussian distribution is a good choice for our likelihood for
repeated measurements, and we will use it for the likelihood for our
present problem of estimating a parameter form repeated measurements.
We have to decide how the measurements are related to specify how many
entries in the covariance matrix we need to specify as parameters.  It
is often the case that the measurements are independent and identically
distributed (i.i.d.), so that only a single variance, $\sigma^2$, is
specified.  So, we choose our likelihood to be
\begin{align}
  P(D\mid \mu, \sigma, I) = \left(\frac{1}{2\pi \sigma^2} \right)^{\frac{n}{2}}\,
  \exp\left\{-\frac{1}{2\sigma^2}\,\sum_{i\in D} (x_i - \mu)^2\right\},
\end{align}
with $n = |D|$.  By choosing this as our likelihood, we are saying
that we expect our measurements to have a well-defined mean $\mu$ with
a spread described by the variance, $\sigma^2$.

But wait a minute; we now have another parameter, $\sigma$, beyond the
one we're trying to measure.  So, our model (model 3) has \textit{two}
parameters, and Bayes's theorem now reads
\begin{align}
P(\mu, \sigma \mid D, I) = \frac{P(D\mid \mu, \sigma, I)\,P(\mu, \sigma \mid I)}
{P(D\mid I)}.
\end{align}
After we compute the posterior, we can still find the probability
distribution we are after by marginalizing.
\begin{align}
P(\mu\mid D, I) = \int_0^\infty \mathrm{d}\sigma\,P(\mu, \sigma \mid D, I).
\end{align}


% %%%%%%%%%%%%%%%%%%%
\subsection{Choice of prior}
Because the evidence $P(D\mid I)$ is entirely determined by the
likelihood, prior, and normalization condition of the posterior, we
need only to specify the likelihood and prior to get the posterior.
We have chosen a Gaussian distribution for our likelihood, so now we
need to specify $P(\mu, \sigma \mid I)$.  The prior encodes what we
know about the parameters \textit{before} the experiments.  The prior
may be informed by previous experiments, as we discussed in section
\ref{sec:l01_learning}.  But what if we know nothing?

One assumption we could make is that $\mu$ and $\sigma$ are
independent.  In this case,
\begin{align}
P(\mu, \sigma \mid I) = P(\mu \mid I)\,P(\sigma\mid I).
\end{align}
Now, we have to decide on prior probabilities for $\mu$ and $\sigma$.
Our goal is to choose \textit{uninformative priors}, i.e., we want to
claim maximal ignorance in our choice of prior probability when we
have to prior information.

For $\mu$, we will choose a uniform prior, meaning that all values are
equally likely.
\begin{align}
P(\mu\mid I) = \left\{\begin{array}{ccl}
\left(\mu_\mathrm{max} - \mu_\mathrm{min}\right)^{-1} & & \mu_\mathrm{min} < \mu < \mu_\mathrm{max}, \\[1em]
0 & & \text{otherwise}
\end{array}\right.
\end{align}
We have put bounds on the allowed values of $\mu$.  You may argue that
this is informative; we have chosen bounds.  We intentionally choose
the bounds to be so far away from the peak of the likelihood that the
posterior is vanishingly small at the bounds.  Thus, the parameters
$\mu_\mathrm{min}$ and $\mu_\mathrm{max}$ are arbitrary and serve only
to ensure the $P(\mu \mid I)$ is a proper probability
distribution.\footnote{There is really no problem with it not being
  proper.  In fact, improper priors are commonly used.}

For the parameter $\sigma$, the choice is not quite so simple.  First,
we know that $\sigma > 0$ by construction.  We also note that we could
have chosen to parametrize the Gaussian distribution with
$\xi \equiv \sigma^{-1}$ instead of $\sigma$.  We want there to be no
bias as a result of this choice.  In other words, we want
\begin{align}
P(\sigma\mid I) = P(\xi \mid I) \, \left|\frac{\mathrm{d}\xi}{\mathrm{d}\sigma}\right|,
\label{eq:l02_jeffreys1}
\end{align}
where we have employed the change of variables
formula.\footnote{Remember your calculus: $f(x) =
  f(y)\left|\mathrm{d}y/\mathrm{d}x\right|$.}  We will choose
\begin{align}
  P(\sigma \mid I) =  \left\{\begin{array}{ccl}
\left(\ln(\sigma_\mathrm{max} / \sigma_\mathrm{min})\,\sigma\right)^{-1} & & \sigma_\mathrm{min} < \sigma < \sigma_\mathrm{max}\\[1em]
0 & & \text{otherwise},
\end{array}\right.
\end{align}
and show that the condition given in equation \eqref{eq:l02_jeffreys1}
hold.  For $\sigma_\mathrm{max}^{-1} < \xi < \sigma_\mathrm{min}^{-1}$, we
have
\begin{align}
P(\xi \mid I) \, \left|\frac{\mathrm{d}\xi}{\mathrm{d}\sigma}\right|
= \ln(\sigma_\mathrm{max} / \sigma_\mathrm{min})\,\xi^{-1} \, \sigma^{-2}
=  \ln(\sigma_\mathrm{max} / \sigma_\mathrm{min})\,\sigma^{-1}
= P(\sigma\mid I),
\end{align}
so the condition holds.  It holds trivially outside of the bounds.
This prior, $P(\sigma\mid I) \propto \sigma^{-1}$, is often called a
\textbf{Jeffreys prior}\footnote{The term \textit{Jeffreys prior} can also be used to describe a class of uninformative priors more generally, but we will not dwell on that here.}, as Harold Jeffreys suggested it in 1939.  It is an uninformative prior to use for
\textbf{scale parameters} like $\sigma$, which can equivalently be
represented by their reciprocals.  Like the prior for $\mu$, the
bounds $\sigma_\mathrm{min}$ and $\sigma_\mathrm{max}$ can be chosen
to be sufficiently large/small such that they are immaterial.

\paragraph{An aside about the Jeffreys prior.} Again using the change
of variables formula, we see that
\begin{align}
P(\sigma \mid I) = P(\ln \sigma \mid I)\left|\frac{\mathrm{d}\ln \sigma}{\mathrm{d}\sigma}\right|
=  \frac{1}{\sigma}\, P(\ln \sigma \mid I).
\end{align}
Thus,
\begin{align}
P(\ln \sigma \mid I) = \left\{\begin{array}{ccl}
\left(\ln \sigma_\mathrm{max} - \ln \sigma_\mathrm{min}\right)^{-1} & & \ln \sigma_\mathrm{min} < \ln \sigma < \ln \sigma_\mathrm{max}, \\[1em]
0 & & \text{otherwise}.
\end{array}\right.
\end{align}
So, if a parameter has a Jeffreys prior, its logarithm has a uniform
prior.  This can be convenient when defining parameters and performing
optimizations.


% %%%%%%%%%%%%%%%%%%%
\subsection{The posterior}
Now that we have specified the likelihood and prior, we have the
posterior.
\begin{align}
P(\mu, \sigma \mid D, I) = \frac{c}{\sigma^{n+1}}\,
\exp\left\{-\frac{1}{2\sigma^2}\,\sum_{i\in D} (x_i - \mu)^2\right\},
\end{align}
where we have absorbed all constants in to the normalization constant
$c$\footnote{We do this here for convenience, but when we do model selection later on, we will have to compute the evidence, so we should be careful about the normalization constants of the priors throughout our calculations.}.  We could do this because we chose the bounds on the priors of
$\mu$ and $\sigma$ to be sufficiently far away from the peak of the
likelihood that they contribute negligibly to the posterior.  This
also allows us to extend our bounds of integration to infinity when
integrating.

So, we are done!  We have now updated our knowledge of $\mu$ and
$\sigma$.  We could just plot the posterior distribution.  We could
show it as a contour plot in the $\mu$-$\sigma$ plane, for instance.

But, it would be nice to get the posterior into a bit of a cleaner
form.  We can show, after some algebraic grunge, that
\begin{align}
\sum_{i\in D}(x_i - \mu)^2 = n(\bar{x} - \mu)^2 + nr^2,
\end{align}
where
\begin{align}
r^2 = \frac{1}{n}\sum_{i\in D}(x_i - \bar{x})^2
\end{align}
is the sample variance and
\begin{align}
\bar{x} = \frac{1}{n}\sum_{i\in D}x_i.
\end{align}
Thus, we have
\begin{align}
P(\mu, \sigma \mid D, I) = \frac{c\,\mathrm{e}^{-nr^2/2\sigma^2}}{\sigma^{n+1}}\,
\exp\left\{-\frac{n(\mu - \bar{x})^2}{2\sigma^2}\right\}.
\end{align}
In this form, we immediately see that, regardless the value of
$\sigma$, the most probable value of $\mu$ is $\bar{x}$.  This is
perhaps not surprising that the most probable value of $\mu$ is the
sample mean, but it is pleasing how nicely it falls out of the
analysis.

Now, it would really like to get a summary of the posterior to be able
to report some nice numbers, like most the probable $\mu = \bar{x}$,
instead of a plot.


% %%%%%%%%%%%%%%%%%%%
\subsubsection{The mean $\mu$}
We wanted to get $P(\mu\mid D, I)$ in the first place.  As we said
before, we can get that by marginalizing over $\sigma$.
\begin{align}
P(\mu\mid D, I) &= \int_0^\infty \mathrm{d}\sigma\,P(\mu, \sigma \mid D, I) \\ \nonumber
&= c\int_0^\infty \frac{\mathrm{d}\sigma}{\sigma^{n+1}}\,
\exp\left\{-\frac{n(\mu - \bar{x})^2 + nr^2}{2\sigma^2}\right\}.
\end{align}
This integral is a little gnarly, but we can evaluate it.  We end up
getting
\begin{align}
P(\mu\mid D, I) \propto \left(1 + \frac{(\mu - \bar{x})^2}{r^2}\right)^{-\frac{n}{2}} \propto \left(\sum_{i\in D} (x_i - \mu)^2\right)^{-\frac{n}{2}}.
\end{align}
I have written the expression in two equivalent forms because it is sometimes more convenient to use one or the other. They are proportional, as you can verify by yourself. For now, we'll use the first expression, since it is convenient for computing the marginalized posteriors.
We can integrate this to get the normalization constant, giving
\begin{align}
P(\mu\mid D, I) = \frac{\Gamma\left(\frac{n}{2}\right)}{\sqrt{\pi}\Gamma\left(\frac{n-1}{2}\right)}\,\frac{1}{r}\,\left(1 + \frac{(\mu - \bar{x})^2}{r^2}\right)^{-\frac{n}{2}}.
\end{align}
The normalization contains gamma functions.  This distribution has a
name.  It is the \textbf{Student-t} distribution.  As we now know, it
describes the estimate mean of a Gaussian distribution with unknown variance from which the data were drawn.  As written, the Student-t
distribution above is said to have $n-1$ degrees of freedom.

As we have already determined, the most probable value of $\mu$ is
$\bar{x}$.  We would like to describe an error bar\footnote{I'm using the term ``error bar'' loosely
  here.  We will sharpen this definition later in the course.} for
this parameter $\mu$.  Since we know its posterior, the error bar is just some summary of the posterior distribution.  We could
report the error bar to contain the set of values of $\mu$,
centered on $\bar{x}$, that contain a given percentage of the
probability.

The common practice for getting the error bar  is to
approximate the posterior distribution as Gaussian and report
intervals based on the standard deviation of the Gaussian
approximation.  To get a Gaussian approximation, we expand the
logarithm of posterior probability distribution function in a Taylor
series about its maximum.
\begin{align}
\ln P(\mu \mid D, I) &= \text{constant} - \frac{n}{2}\,\ln\left(1 + \frac{(\mu - \bar{x})^2}{r^2}\right) \\
&\approx \text{constant} - \frac{n(\mu - \bar{x})^2}{r^2}.
\end{align}
Exponentiating and evaluating the normalization constant yields
\begin{align}
  P(\mu\mid D, I) \approx \frac{1}{\sqrt{2\pi r^2/n}}\,\exp\left\{
-\frac{(\mu - \bar{x})^2}{2r^2/n}
\right\},
\end{align}
a Gaussian distribution with mean $\bar{x}$ and variance $r^2/n$.
Recall that $r^2$ is the sample variance, so the variance of the
Gaussian approximation of the posterior distribution is the sample
variance divided by $n$.  The quantity $r/\sqrt{n}$ is referred to as
the \textbf{standard error of the mean}, which is often how error bars
are reported.  We now know that it describes the width of the
(Gaussian approximation of the) posterior distribution describing the
parameter value we sought to measure.

% %%%%%%%%%%%%%%%%%%%
\subsubsection{The variance $\sigma^2$}
Often overlooked is an estimate for the variance.  Remember, when we
took measurements, we did not assume we knew the variance of the
measurements.  We would also like an estimate of it.

We take a similar approach.  We marginalize the full posterior over
$\mu$.
\begin{align}
P(\sigma\mid D, I) = \int_{-\infty}^\infty \mathrm{d}\mu\,P(\mu,\sigma\mid D, I).
\end{align}
The integral is again doable, but also again a bit gnarly.  The result is
\begin{align}
P(\sigma\mid D, I) = \frac{c}{\sigma^n}\,\exp\left\{-\frac{nr^2}{2\sigma^2}\right\}.
\end{align}
We can compute the normalization constant, but it is a messy
expression including some incomplete gamma functions.  We will not
bother.  Instead, we can find the most probable $\sigma$.  This is
found by finding the value of $\sigma$ for which the derivative of the
posterior is zero.
\begin{align}
  \frac{\mathrm{d}}{\mathrm{d}\sigma}P(\sigma \mid D, I) &=
  c(nr^2\sigma^{-n-3} - n \sigma^{-n-1})\exp\left\{-\frac{nr^2}{2\sigma^2}\right\} \\
  &= c n \sigma^{-n-1}\left(\frac{r^2}{\sigma^2} - 1\right)\exp\left\{-\frac{nr^2}{2\sigma^2}\right\}.
\end{align}
This is zero when $\sigma^2 = r^2$, or
\begin{align}
    \sigma^2 = \frac{1}{n}\sum_{i\in D}(x_i - \bar{x})^2.
\end{align}

We can also compute a confidence interval on the parameter $\sigma$.
Note, though, that its distribution, $P(\sigma \mid D, I)$, is not
symmetric, as seen in Fig.~\ref{fig:l02_sigma_post}.

\begin{figure}[h]
\centerline{
        \includegraphics[width=0.9\linewidth]{figs/sigma_posterior.pdf}}
      \caption{The posterior distribution of $\sigma$ with $r = 1$ for
        various values of $n$.  It becomes more symmetric as $n$ grows.}
\label{fig:l02_sigma_post}
\end{figure}

Given that the distribution is not symmetric, we might want to provide a point estimate for $\sigma$ using expectation values, instead of finding the most probable value. The integrals are nasty, but can be evaluated.
\begin{align}
    \langle \sigma \rangle = \int_0^\infty \mathrm{d}\sigma\,\sigma\,P(\sigma\mid D, I)
     = \frac{\Gamma\left(\frac{n-2}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)}\,\sqrt{\frac{n}{2}}\,r.
\end{align}
Alternatively, we could compute the expectation value for $\sigma^2$,
\begin{align}
    \langle \sigma^2 \rangle = \int_0^\infty \mathrm{d}\sigma\,\sigma^2\,P(\sigma\mid D, I)
     = \frac{n}{n-1}\,r^2 = \frac{1}{n-1}\sum_{i \in D}(x_i - \bar{x})^2,
 \end{align}
which may be familiar to you as the so-called sample variance, or the unbiased estimate of the variance. Really, by choosing to report the most probable value of $\sigma$, the $\langle \sigma \rangle$, or $\sqrt{\langle \sigma^2\rangle}$, we are just choosing one property of $P(\sigma\mid D, I)$ to report. We actually know the whole distribution, though, so whatever we choose is just a summary of it.
