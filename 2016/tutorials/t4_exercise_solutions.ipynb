{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: exercise\n",
    "\n",
    "(c) 2016 Justin Bois. This work is licensed under a [Creative Commons Attribution License CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/). All code contained herein is licensed under an [MIT license](https://opensource.org/licenses/MIT).\n",
    "\n",
    "*This tutorial exercise was generated from an Jupyter notebook.  You can download the notebook [here](t4_exercise_solutions.ipynb).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Describe how the some of the nonlinear regression we did this past week is a maximum likelihood estimation.\n",
    "\n",
    "#### Answer\n",
    "\n",
    "In the nonlinear regression we did last week, the likelihood was Gaussian distributed about the mathematical model.\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mathbf{a}\\mid \\{x_i,y_i\\}, I) \\propto P(\\{x_i,y_i\\}\\mid\\mathbf{a}, I)\\,P(\\mathbf{a},I)\n",
    "\\propto \\sigma^{-(n+1)} \\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i\\in D} \\left(y_i - y(x_i;\\mathbf{a})\\right)^2\\right],\n",
    "\\end{align}\n",
    "\n",
    "where we have assumed a Jeffreys prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Why couldn't we use `scipy.optimize.leastsq()` on the Singer data from [Tutorial 4]()?\n",
    "\n",
    "#### Answer\n",
    "\n",
    "`scipy.optimize.leastsq()` uses the Levenberg-Marquardt algorithm, which minimized objective functions that are the sum of squares. The posterior distribution of the Singer data contains a Negative Binomial likelihood, so the problem of finding the MAP cannot be cast into a sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Why is it important to \"burn in\" walkers when performing a MCMC calculation?\n",
    "\n",
    "#### Answer\n",
    "\n",
    "The walkers need to achieve stationarity in order to be sampling the correct distribution. The burn-in period gives them time to do so. Keeping the burn-in samples would result in keeping samples that are not our of the target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Say we used MCMC to sample a posterior distribution that had 6 parameters, $P(a_1, a_2, a_3, a_4, a_5, a_6 \\mid D, I)$. From the MCMC samples, how can we get samples for the marginalized distribution $P(a_3 \\mid D, I)$?\n",
    "\n",
    "#### Answer\n",
    "\n",
    "We simple ignore the samples from the other variables!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
